<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Release Notes - OpenMPF</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Release Notes";
    var mkdocs_page_input_path = "Release-Notes.md";
    var mkdocs_page_url = "/Release-Notes/index.html";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../index.html" class="icon icon-home"> OpenMPF</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../index.html">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">General</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="index.html">Release Notes</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#openmpf-70x">OpenMPF 7.0.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-63x">OpenMPF 6.3.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-62x">OpenMPF 6.2.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-61x">OpenMPF 6.1.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-60x">OpenMPF 6.0.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-51x">OpenMPF 5.1.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-50x">OpenMPF 5.0.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-41x">OpenMPF 4.1.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-40x">OpenMPF 4.0.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-30x">OpenMPF 3.0.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-21x">OpenMPF 2.1.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-20x">OpenMPF 2.0.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-10x">OpenMPF 1.0.x</a></li>
    

    <li class="toctree-l3"><a href="#openmpf-0xx">OpenMPF 0.x.x</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../License-And-Distribution/index.html">License and Distribution</a>
                </li>
                <li class="">
                    
    <a class="" href="../Acknowledgements/index.html">Acknowledgements</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Guides</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Install-Guide/index.html">Install Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Admin-Guide/index.html">Admin Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../User-Guide/index.html">User Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Media-Segmentation-Guide/index.html">Media Segmentation Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Feed-Forward-Guide/index.html">Feed Forward Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Derivative-Media-Guide/index.html">Derivative Media Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Object-Storage-Guide/index.html">Object Storage Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Markup-Guide/index.html">Markup Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../REST-API/index.html">REST API</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Component Development</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Component-API-Overview/index.html">Component API Overview</a>
                </li>
                <li class="">
                    
    <a class="" href="../Component-Descriptor-Reference/index.html">Component Descriptor Reference</a>
                </li>
                <li class="">
                    
    <a class="" href="../CPP-Batch-Component-API/index.html">C++ Batch Component API</a>
                </li>
                <li class="">
                    
    <a class="" href="../Python-Batch-Component-API/index.html">Python Batch Component API</a>
                </li>
                <li class="">
                    
    <a class="" href="../Java-Batch-Component-API/index.html">Java Batch Component API</a>
                </li>
                <li class="">
                    
    <a class="" href="../GPU-Support-Guide/index.html">GPU Support Guide</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Additional Developer Resources</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Contributor-Guide/index.html">Contributor Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Development-Environment-Guide/index.html">Development Environment Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Node-Guide/index.html">Node Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../Workflow-Manager-Architecture/index.html">Workflow Manager Architecture</a>
                </li>
                <li class="">
                    
    <a class="" href="../CPP-Streaming-Component-API/index.html">C++ Streaming Component API</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">OpenMPF</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
    
      
        
          <li>General &raquo;</li>
        
      
    
    <li>Release Notes</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p><strong>NOTICE:</strong> This software (or technical data) was produced for the U.S. Government under contract, and is subject to the
Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2021 The MITRE Corporation. All Rights Reserved.</p>
<h1 id="openmpf-70x">OpenMPF 7.0.x</h1>
<h2>7.0.0: July 2022</h2>

<h3>Documentation</h3>

<ul>
<li>Updated the Development Environment Guide by replacing steps for CentOS 7 with Ubuntu 20.04.</li>
<li>Added the Derivative Media Guide.</li>
<li>Updated the Batch Component APIs with revised error codes.</li>
<li>Updated the Python Batch Component API and Python base Docker image README with instructions for
  using <code>pyproject.toml</code> and <code>setup.cfg</code>.</li>
<li>Updated the Admin Guide and User Guide with images that show the new TiesDb and Callback columns in the job status UI.</li>
<li>Updated the REST API with the <code>pipelineDefinition</code>, <code>frameRanges</code>, and <code>timeRanges</code> fields now supported by the
  <code>[POST] /rest/jobs</code> endpoint.</li>
<li>Updated the OcvYoloDetection component README with information on using the NVIDIA Triton inference server.</li>
<li>Updated the Contributor Guide with new steps for generating documentation.</li>
</ul>
<h3>Transition from CentOS 7 to Ubuntu 20.04</h3>

<ul>
<li>All the Docker images that previously used CentOS 7 as a base now use Ubuntu 20.04.</li>
<li>We decided not to use CentOS 8, which is a version of CentOS Stream, due to concerns about stability.</li>
<li>Also, Ubuntu is a very common OS within the AI and ML space, and has significant community support.</li>
</ul>
<h3>Use Job Id that Enables Load Balancing</h3>

<ul>
<li>The Workflow Manager can now optionally accept job ids of the form <code>&lt;openmpf-instance-wfm-hostname&gt;-&lt;job-id&gt;</code> through
  the REST endpoints, where <code>&lt;job-id&gt;</code> is the same as the shorter id used in previous releases. The
  <code>&lt;openmpf-instance-wfm-hostname&gt;-</code> prefix enables better tracking and separation of jobs run across multiple
  Workflow Manager instances in a cluster.</li>
<li>The prefix can be set in the <code>docker-compose.yml</code> file by assigning <code>{{.Node.Hostname}}</code> to the <code>NODE_HOSTNAME</code>
  environment variable for the Workflow Manager service, or hard-coding <code>NODE_HOSTNAME</code> to the desired hostname.</li>
<li>The shorter version of the id can still be used in REST requests, but the longer id will always be returned by the
  Workflow Manager when responding to those requests.</li>
<li>The shorter id will always be used internally by the Workflow Manager, meaning the Job Status web UI and log messages
  will all use the shorter job id. </li>
</ul>
<h3>Support for Derivative Media</h3>

<ul>
<li>The TikaImageDetection component now returns <code>MEDIA</code> tracks instead of <code>IMAGE</code> tracks when extracting images from
  documents, such as PDFs, Word documents, and PowerPoint slides. The document is considered the "source", or "parent",
  media, and the images are considered the "derivative", or "child", media.</li>
<li>Actions can now be configured with <code>SOURCE_MEDIA_ONLY=true</code> or <code>DERIVATIVE_MEDIA_ONLY=true</code>, which will result in only
  performing the action on that kind of media. Feed forward can still be used to pass track information from one stage
  to another. The tracks will skip the stages (actions) that don't apply.</li>
<li>This enables complex pipelines like one that extracts text from a PDF using TikaTextDetection, OCRs embedded images
  using EastTextDetection and TesseractOCRTextDetection, and runs all of the <code>TEXT</code> tracks through KeywordTagging.</li>
</ul>
<h3>Report when Job Callbacks and TiesDb POSTs Fail</h3>

<ul>
<li>The job status UI displays two new columns, one that indicates the status of posting to TiesDB, and one that indicates
  the status of posting the job callback to the job producer.</li>
<li>Additionally, the <code>[GET] /rest/jobs/{id}</code> endpoint now includes a <code>tiesDbStatus</code> and <code>callbackStatus</code> field.</li>
<li>Note that, by design, the JSON output itself does not contain these statuses.</li>
</ul>
<h3>Allow Pipelines to be Specified in a Job Request</h3>

<ul>
<li>Optionally, the <code>pipelineDefinition</code> field can be provided instead of the <code>pipelineName</code> field when using the
  <code>[POST] /rest/jobs</code> endpoint in order to specify a pipeline on the fly for that specific job run. It will not be saved
  for later reuse.</li>
<li>The format of the pipeline definition is similar to that in a <code>descriptor.json</code> file, with separate sections for
  defining <code>tasks</code> and <code>actions</code>. Pre-existing tasks and actions known to the Workflow Manager can be specified in the
  definition. They do not need to be defined again.</li>
<li>This feature is a convenient alternative to creating persistent definitions using the <code>[POST] /rest/pipelines</code>,
  <code>[POST] /rest/tasks</code>, and <code>[POST] /rest/actions</code> endpoints. For example, to quickly add or remove a motion
  preprocessing stage from a pipeline.</li>
</ul>
<h3>Allow User-Specified Segment Boundaries</h3>

<ul>
<li>Optionally, multiple <code>frameRanges</code> and/or <code>timeRanges</code> fields can be provided when using the <code>[POST] /rest/jobs</code>
  endpoint in order to manually specify segment boundaries. These values will override the normal segmenting behavior of
  the Workflow Manager.</li>
<li>Not that overlapping ranges will be combined and large ranges may still be split up according to the value of
  <code>TARGET_SEGMENT_LENGTH</code> and <code>VFR_TARGET_SEGMENT_LENGTH</code>.</li>
<li>Note that <code>frameRanges</code> is specified using the frame number and <code>timeRanges</code> is specified in milliseconds.</li>
</ul>
<h3>Add Triton Inference Server support to YOLO component</h3>

<ul>
<li>The OcvYoloDetection component now supports the ability to send requests to an NVIDIA Triton Inference Server by
  setting <code>ENABLE_TRITON=true</code>. If set to false, the component will process jobs using OpenCV DNN on the local host
  running the Docker service, as per normal.</li>
<li>By default <code>TRITON_SERVER=ocv-yolo-detection-server:8001</code>, which
  corresponds to the <code>ocv-yolo-detection-server</code> entry in your <code>docker-compose.yml</code> file. Refer to the example entry
  within <a href="https://github.com/openmpf/openmpf-docker/blob/develop/docker-compose.components.yml"><code>docker-compose.components.yml</code></a>
  . That entry uses a pre-built and pre-configured version of the Triton server.</li>
<li>The Triton server runs the YOLOv4 model within the TensorRT framework, which performs a warmup operation when the
  server starts up to determine which optimizations to enable for the available GPU hardware. <code>*.engine</code> files are
  generated within the <code>yolo_engine_file</code> Docker volume for later reuse.</li>
<li>To further improve inferencing speed, shared memory can be configured between the <code>ocv-yolo-detection</code> client service and the
  <code>ocv-yolo-detection-server</code> service if they are running on the same host. Set <code>TRITON_USE_SHM=true</code> and configure the
  server with a <code>/dev/shm:/dev/shm</code> Docker volume.</li>
<li>Depending on the available GPU hardware, the Triton server can achieve speeds that are 5x faster than OpenCV DNN with
  tracking enabled, no shared memory, and nearly 9x faster with tracking disabled, with shared memory. Our tests used a
  single RTX 2080 GPU.</li>
</ul>
<h3>Removed Unused and Redundant Error Codes</h3>

<ul>
<li>The error codes shown on the left were redundant and replaced with the corresponding error codes on the right:</li>
</ul>
<table>
<thead>
<tr>
<th>Old Error Code</th>
<th>New Error Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>MPF_IMAGE_READ_ERROR</td>
<td>MPF_COULD_NOT_READ_MEDIA</td>
</tr>
<tr>
<td>MPF_BOUNDING_BOX_SIZE_ERROR</td>
<td>MPF_BAD_FRAME_SIZE</td>
</tr>
<tr>
<td>MPF_JOB_PROPERTY_IS_NOT_INT</td>
<td>MPF_INVALID_PROPERTY</td>
</tr>
<tr>
<td>MPF_JOB_PROPERTY_IS_NOT_FLOAT</td>
<td>MPF_INVALID_PROPERTY</td>
</tr>
<tr>
<td>MPF_INVALID_FRAME_INTERVAL</td>
<td>MPF_INVALID_PROPERTY</td>
</tr>
<tr>
<td>MPF_DETECTION_TRACKING_FAILED</td>
<td>MPF_OTHER_DETECTION_ERROR_TYPE</td>
</tr>
</tbody>
</table>
<p>Also, the following error codes are no longer being used and have been removed:</p>
<ul>
<li><code>MPF_UNRECOGNIZED_DATA_TYPE</code><ul>
<li>All media types can now be processed since we support the <code>UNKNOWN</code> (a.k.a. "generic")
  media type</li>
</ul>
</li>
<li><code>MPF_INVALID_DATAFILE_URI</code><ul>
<li>The Workflow Manager will reject a job with an invalid media URI before it gets to a
  component</li>
</ul>
</li>
<li><code>MPF_INVALID_START_FRAME</code></li>
<li><code>MPF_INVALID_STOP_FRAME</code></li>
<li><code>MPF_INVALID_ROTATION</code></li>
</ul>
<h3>Markup Improvements</h3>

<ul>
<li>By default, the Markup component draws bounding boxes to fill in the gaps between detections in each track by
  interpolating the box size and position. This can now be disabled by setting the job property
  <code>MARKUP_ANIMATION_ENABLED=false</code>, or the system property <code>markup.video.animation.enabled=false</code>.
  Disabling this feature can be useful to prevent floating boxes from cluttering the marked-up frames.</li>
<li>The Markup component will now start each bounding box label with a track index like <code>[0]</code> that can be used to
  correlate the box with the track in the JSON output object. The JSON output now contains an <code>index</code> field for every
  track, relative to each piece of media, that is simply an integer that starts at 0 and counts upward. This can be
  disabled by setting the job property <code>MARKUP_LABELS_TRACK_INDEX_ENABLED=false</code>, or the system property
  <code>markup.labels.track.index.enabled=false</code>.</li>
</ul>
<h3>Changes to JSON Output Object</h3>

<ul>
<li>Components that generate <code>MEDIA</code> tracks will result in new derivative <code>media</code> entries in the JSON output file. This
  means it's possible to provide a single piece of media as in input and have more than one <code>media</code> entry in the JSON
  output. The output will always include the original media.</li>
<li>Each <code>media</code> entry in the JSON output now contains a <code>parentMediaId</code> in addition to the <code>mediaId</code>. The <code>parentMediaId</code>
  for original source media will always be set to -1; otherwise, for derivative media, the <code>parentMediaId</code> is set the
  <code>mediaId</code> of the source media from which the child media was derived.</li>
<li>Each <code>media</code> entry also contains a new <code>frameRanges</code> and <code>timeRanges</code> collection.</li>
<li>The JSON output file also contains a new <code>index</code> field for every track, relative to each piece of media.</li>
</ul>
<h3>Features</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/792">#792</a>] Perform detection on images extracted from PDFs</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1283">#1283</a>] Add user-specified segment boundaries</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1374">#1374</a>] Transition from CentOS 7 to Ubuntu 20.04</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1396">#1396</a>] Report when job callbacks and TiesDb POSTs fail</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1398">#1398</a>] Add Triton Inference Server support to YOLO component</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1428">#1428</a>] Allow pipelines to be specified in a job request</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1454">#1454</a>] Transition from Clair scans to Trivy scans</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1485">#1485</a>] Use <code>pyproject.toml</code> and <code>setup.cfg</code> instead of <code>setup.py</code></li>
</ul>
<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/803">#803</a>] Update Tika Image Detection to generate one track per piece of extracted media</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/808">#808</a>] Update Tika Text Detection component to not use leading zeros for <code>PAGE_NUM</code></li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1105">#1105</a>] Remove dependency on QT from C++ SDK</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1282">#1282</a>] Use job id that enables load balancing</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1303">#1303</a>] Update Tika Image Detection to return <code>MEDIA</code> tracks</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1319">#1319</a>] Review existing error codes and remove unused or redundant error codes</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1384">#1384</a>] Update Apache Tika to 2.4.1 for TikaImageDetection and TikaTextDetection Components</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1436">#1436</a>] CLI Runner should initialize a component once when handling multiple jobs</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1465">#1465</a>] Remove YoloV3 support from OcvYoloDetection component</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1513">#1513</a>] Update to Spring 5.3.18</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1528">#1528</a>] CLI runner should also sort by startOffsetTime</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1540">#1540</a>] Upgrade to Java 17</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1549">#1549</a>] Allow markup animation to be disabled</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1550">#1550</a>] Add track index to markup</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1372">#1372</a>] Tika Image Detection no longer misses images in PowerPoint and Word documents</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1449">#1449</a>] Simon data is now refreshed when clicking the Processes tab</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1495">#1495</a>] Fix bug where invalid CSRF token found for <code>/workflow-manager/login</code></li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1501">#1501</a>] MPFVideoCapture now properly reads frames from videos with rotation metadata</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1502">#1502</a>] Detections with <code>HORIZONTAL_FLIP</code> can result in illformed detections and incorrectly padded regions</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1503">#1503</a>] Videos with rotation metadata can result in corrupt markup</li>
</ul>
<h1 id="openmpf-63x">OpenMPF 6.3.x</h1>
<h2>6.3.14: May 2022</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1530">#1530</a>] Fix S3 code memory leak</li>
</ul>
<h2>6.3.12: April 2022</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1519">#1519</a>] Upgrade to OpenCV 4.5.5</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1520">#1520</a>] S3 code now retries on most 400 errors</li>
</ul>
<h2>6.3.11: April 2022</h2>

<h3>Documentation</h3>

<ul>
<li>Updated the Object Storage Guide with <code>S3_SESSION_TOKEN</code>, <code>S3_USE_VIRTUAL_HOST</code>, <code>S3_HOST</code>, and <code>S3_REGION</code>.</li>
</ul>
<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1496">#1496</a>] Update S3 client code</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1514">#1514</a>] Update Tomcat to 8.5.78</li>
</ul>
<h2>6.3.10: March 2022</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1486">#1486</a>] Fix bug where <code>MOVING</code> was being added to immutable map twice</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1498">#1498</a>] Can now provide media metadata when frameTimeInfo is missing</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1501">#1501</a>] MPFVideoCapture now properly reads frames from videos with rotation metadata</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1502">#1502</a>] Detections with <code>HORIZONTAL_FLIP</code> will no longer result in illformed detections and incorrectly padded regions</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1503">#1503</a>] Videos with rotation metadata will no longer result in corrupt markup</li>
</ul>
<h2>6.3.8: January 2022</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1469">#1469</a>] <code>TENSORFLOW VEHICLE COLOR DETECTION</code> pipelines no longer refer to YOLO tasks that no longer exist</li>
</ul>
<h2>6.3.7: January 2022</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1466">#1466</a>] Upgrade log4j to 2.17.1</li>
</ul>
<h2>6.3.6: December 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1457">#1457</a>] Upgrade log4j to 2.16.0</li>
</ul>
<h2>6.3.5: November 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1451">#1451</a>] Make concurrent callbacks configurable</li>
</ul>
<h2>6.3.4: November 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1441">#1441</a>] Modify AdminStatisticsController so that it doesn't hold all jobs in memory at once</li>
</ul>
<h2>6.3.3: October 2021</h2>

<h3>Features</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1425">#1425</a>] Make protobuf size limit configurable</li>
</ul>
<h2>6.3.2: October 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1420">#1420</a>] Sphinx component no longer omits audio at end of video files</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1422">#1422</a>] Media inspection now correctly calculates milliseconds from ffmpeg duration</li>
</ul>
<h2>6.3.1: September 2021</h2>

<h3>Features</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1404">#1404</a>] Improve OcvDnnDetection vehicle color detection</li>
</ul>
<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1251">#1251</a>] Add version to JSON output object</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1272">#1272</a>] Update Keyword Tagging to work on multiple inputs</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1350">#1350</a>] Retire old components to the graveyard: DlibFaceDetection, DarknetDetection, and OcvPersonDetection</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1010">#1010</a>] <code>mpf.output.objects.enabled</code> now behaves as expected</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1271">#1271</a>] Azure speech component no longer omits audio at end of video files</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1389">#1389</a>] NLP text correction component now properly reads the value of <code>FULL_TEXT_CORRECTION_OUTPUT</code></li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1403">#1403</a>] Corrected README to state that the Azure Speech Component doesn't support v2 of the API</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1406">#1406</a>] Speech detections in videos are no longer dropped if using keyword tagging</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1411">#1411</a>] Exception no longer occurs when adding <code>SHRUNK_TO_NOTHING=TRUE</code> to an immutable map in multiple pipeline stages</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1413">#1413</a>] Speech detections in videos are no longer dropped if using translation</li>
</ul>
<h2>6.3.0: September 2021</h2>

<h3>Documentation</h3>

<ul>
<li>Updated the API documents, Development Environment Guide, Node Guide, Install Guide, User Guide, Admin Guide, and
  others to clarify the difference between Docker and non-Docker behaviors.</li>
<li>Transformed Packaging and Registering a Component document into Component Descriptor Reference.</li>
<li>Split Media Segmentation Guide from User Guide.</li>
<li>Updated and renamed the Workflow Manager document to Workflow Manager Architecture.</li>
<li>Updated the various Docker guides to clarify the difference between building Docker images from scratch versus
  building them using pre-built base images on Docker Hub, emphasizing the latter.</li>
<li>Updated the Contributor Guide to document the hotfix pull request process.</li>
</ul>
<h3>TiesDb Integration</h3>

<ul>
<li>TiesDb is a PostgreSQL DB with a RESTful API that stores media metadata. The metadata entries are queried using the
  hash (sha256, md5) of the media file. TIES stands
  for <a href="https://github.com/Noblis/ties-lib">Triage Import Export Schema</a>. TiesDb is deployed and managed externally to
  OpenMPF. For more information please contact us.</li>
<li>When a job completes, OpenMPF can post assertions to media entries that exist in TiesDb. In general, one assertion is
  generated for each algorithm run on a piece of media. It contains the job status, algorithm name, detection
  type (<code>FACE</code>, <code>TEXT</code>, <code>MOTION</code>, etc.), and number of tracks generated, as well as a link to the full JSON output
  object.</li>
<li>Each assertion serves as a lasting record so that job producers may first check TiesDb to see if an algorithm was run
  on a piece of media before submitting the same job to OpenMPF again.</li>
<li>To enable TiesDb support, set the <code>TIES_DB_URL</code> job property or <code>ties.db.url</code> system property to
  the <code>&lt;http|https&gt;://&lt;host&gt;:&lt;port&gt;</code> part of the URL. The Workflow Manager will append
  the <code>/api/db/supplementals?sha256Hash=&lt;hash&gt;</code> part. Here is an example of a TiesDb POST:</li>
</ul>
<pre><code class="language-json">{
  &quot;dataObject&quot;: {
    &quot;sha256OutputHash&quot;: &quot;1f8f2a8b2f5178765dd4a2e952f97f5037c290ee8d011cd7e92fb8f57bc75f17&quot;,
    &quot;outputType&quot;: &quot;FACE&quot;,
    &quot;algorithm&quot;: &quot;FACECV&quot;,
    &quot;processDate&quot;: &quot;2021-09-09T21:37:30.516-04:00&quot;,
    &quot;pipeline&quot;: &quot;OCV FACE DETECTION PIPELINE&quot;,
    &quot;outputUri&quot;: &quot;file:///home/mpf/git/openmpf-projects/openmpf/trunk/install/share/output-objects/1284/detection.json&quot;,
    &quot;jobStatus&quot;: &quot;COMPLETE&quot;,
    &quot;jobId&quot;: 1284,
    &quot;systemVersion&quot;: &quot;6.3&quot;,
    &quot;trackCount&quot;: 1,
    &quot;systemHostname&quot;: &quot;openmpf-master&quot;
  },
  &quot;system&quot;: &quot;OpenMPF&quot;,
  &quot;securityTag&quot;: &quot;UNCLASSIFIED&quot;,
  &quot;informationType&quot;: &quot;OpenMPF FACE&quot;,
  &quot;assertionId&quot;: &quot;4874829f666d79881f7803207c7359dc781b97d2c68b471136bf7235a397c5cd&quot;
}
</code></pre>
<h3>Natural Language Processing (NLP) Text Correction Component</h3>

<ul>
<li>This component utilizes the <a href="https://github.com/MSeal/cython_hunspell">CyHunspell</a> library, which is a Python
  port of the <a href="https://github.com/hunspell/hunspell">Hunspell</a> spell-checking library, to perform post-processing
  correction of OCR text. In general, it's intended to be used in a pipeline after a component like
  TesseractOCRTextDetection that generates <code>TEXT</code> tracks. These tracks are then fed-forward into NlpTextCorrection,
  which will add a <code>CORRECTED TEXT</code> property to the existing tracks.
  The <code>TESSERACT OCR TEXT DETECTION WITH NLP TEXT CORRECTION PIPELINE</code> performs this behavior. The component can also
  run on its own to process plain text files. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/tree/master/python/NlpTextCorrection#readme">README</a> for details.</li>
</ul>
<h3>Azure Cognitive Services (ACS) Read Component</h3>

<ul>
<li>This component utilizes
  the <a href="https://westcentralus.dev.cognitive.microsoft.com/docs/services/computer-vision-v3-1-ga/operations/5d986960601faab4bf452005">Azure Cognitive Services Read Detection REST endpoint</a>
  to extract formatted text from documents (PDFs), images, and videos. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/tree/master/python/AzureReadTextDetection#readme">README</a> for
  details.</li>
</ul>
<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1151">#1151</a>] Now supports <code>IN_PROGRESS_WITH_WARNINGS</code> status</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1234">#1234</a>] Now sorts JSON output object media by media id</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1341">#1341</a>] Added job id to all batch-job-specific Workflow Manager log
  messages</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1349">#1349</a>] Improved reporting and recording job status</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1353">#1353</a>] Updated the Workflow Manager to remove and warn about
  zero-size detections</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1382">#1382</a>] Updated Tika version to 1.27 for TikaImageDetection and
  TikaTextDetection components</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1387">#1387</a>] Markup can now be configured in a
  component's <code>descriptor.json</code></li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1080">#1080</a>] Batch jobs no longer prematurely set to 100% completion
  during artifact extraction</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1106">#1106</a>] When a job ends in <code>ERROR</code> or <code>CANCELLED_BY_SHUTDOWN</code> the
  job status UI now shows an End Date</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1158">#1158</a>] JSON output object URI no longer changes when callback fails</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1317">#1317</a>] TikaTextDetection no longer generates first PDF track
  at <code>PAGE_NUM</code> 2</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1337">#1337</a>] Now using <code>MPF_BAD_FRAME_SIZE</code> instead
  of <code>MPF_DETECTION_FAILED</code> for OpenCV empty/resize exception</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1359">#1359</a>] Image detection tracks no longer
  have <code>endOffsetFrameInclusive</code> set to 1</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1373">#1373</a>] When uploading large files through the Workflow Manager web
  UI, now more than the first 865032704 bytes get written</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1379">#1379</a>] TikaImageDetection component now avoids conflicts by no
  longer using the same path when extracting images for jobs with multiple pieces of media</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1386">#1386</a>] FeedForwardFrameCropper in the Python SDK now handles
  negative coordinates properly</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1391">#1391</a>] If a job is configured to upload markup and markup fails,
  the job no longer gets stuck</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1372">#1372</a>] TikaImageDetection misses images in PowerPoint and Word
  documents</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1389">#1389</a>] NlpTextCorrection does not properly read the value
  of <code>FULL_TEXT_CORRECTION_OUTPUT</code></li>
</ul>
<h1 id="openmpf-62x">OpenMPF 6.2.x</h1>
<h2>6.2.5: July 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1367">#1367</a>] Enable cross-origin resource sharing on Workflow Manager</li>
</ul>
<h2>6.2.4: June 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1356">#1356</a>] AzureSpeech now properly reports when media is missing audio stream</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1357">#1357</a>] AzureSpeech now handles case where speaker id is not present</li>
</ul>
<h2>6.2.2: June 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1333">#1333</a>] Combine media name and job id into one WFM log line</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1336">#1336</a>] Remove duplicate "Setting status of job to COMPLETE" Workflow Manager log line and other improvements</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1338">#1338</a>] Update OpenCV DNN Detection component to optionally use feed-forward confidence values</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1237">#1237</a>] Fixed jQuery DataTables bug: "int parameter 'draw' is present but cannot be translated into a null value"</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1254">#1254</a>] Jobs table no longer flickers when polling is enabled and the search box is used</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1308">#1308</a>] Prevent OCV YOLO Tracking from generating zero-sized detections</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1313">#1313</a>] Fix JSON output object timestamps for variable frame rate videos</li>
</ul>
<h2>6.2.1: May 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1330">#1330</a>] Return error codes for <code>models_ini_parser.py</code> exceptions</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1331">#1331</a>] Decoding certain heic images no longer causes Workflow Manager to segfault</li>
</ul>
<h2>6.2.0: May 2021</h2>

<h3>Tesseract OCR Text Detection Component Support for Videos</h3>

<ul>
<li>The component can now process videos in addition to images and PDFs. Each video frame is processed sequentially.
  The <code>MAX_PARALLEL_SCRIPT_THREADS</code> property determines how many threads to use to process each frame, one thread per
  language or script.</li>
<li>Note that for videos without much text, it may be faster to disable threading by
  setting <code>MAX_PARALLEL_SCRIPT_THREADS=1</code>. This will allow the component to reuse TessAPI instances instead of creating
  new ones for every frame. Please refer to the Known Issues section.</li>
<li>Resolved issues: <a href="https://github.com/openmpf/openmpf/issues/1285">#1285</a></li>
</ul>
<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1086">#1086</a>] Added support for <code>COULD_NOT_OPEN_MEDIA</code>
  and <code>COULD_NOT_READ_MEDIA</code> error types</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1159">#1159</a>] Split <code>IssueCodes.REMOTE_STORAGE</code>
  into <code>REMOTE_STORAGE_DOWNLOAD</code> and <code>REMOTE_STORAGE_UPLOAD</code></li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1250">#1250</a>] Modified <code>/rest/jobs/{id}</code> to include the job's media</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1312">#1312</a>] Created <code>NETWORK_ERROR</code> error code for when a component
  can't connect to an external server. Updated Python HTTP retry code to return <code>NETWORK_ERROR</code>. This affects the Azure
  components.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1008">#1008</a>] Use global TessAPI instances with parallel processing</li>
</ul>
<h1 id="openmpf-61x">OpenMPF 6.1.x</h1>
<h2>6.1.6: May 2021</h2>

<h3>Handle Variable Frame Rate Videos</h3>

<ul>
<li>The Workflow Manager will attempt to detect if a video is constant frame rate (CFR) or variable frame rate (VFR)
  during media inspection. If no determination can be made, it will default to VFR behavior. If CFR, the JSON output
  object will have a <code>HAS_CONSTANT_FRAME_RATE=true</code> property in the <code>mediaMetadata</code> field.</li>
<li>When <code>MPFVideoCapture</code> handles a CFR video it will use OpenCV to set the frame position, unless the position is within
  16 frames of the current position, in which case it will iteratively use OpenCV <code>grab()</code> to advance to the desired
  frame.</li>
<li>When <code>MPFVideoCapture</code> handles a VFR video it will always iteratively use OpenCV <code>grab()</code> to advance to the desired
  frame because setting the frame position directly has been shown to not work correctly on VFR videos.</li>
<li>When a video is split into multiple segments, <code>MPFVideoCapture</code> must iteratively use <code>grab()</code> to advance from frame 0
  to the start of the segment. This introduces performance overhead. To mitigate this we recommend using larger video
  segments than those used for CFR videos.</li>
<li>In addition to the existing <code>TARGET_SEGMENT_LENGTH</code> and <code>MIN_SEGMENT_LENGTH</code> job
  properties (<code>detection.segment.target.length</code> and <code>detection.segment.minimum.length</code> system properties) for CFR
  videos, the Workflow Manager now supports the <code>VFR_TARGET_SEGMENT_LENGTH</code> and <code>VFR_MIN_SEGMENT_LENGTH</code> job
  properties (<code>detection.vfr.segment.target.length</code> and <code>detection.vfr.segment.minimum.length</code> system properties) for
  VFR videos.</li>
<li>Note that the timestamps associated with tracks and detections in a VFR video may be wrong. Please refer to the Known
  Issues section.</li>
<li>Resolved issues: <a href="https://github.com/openmpf/openmpf/issues/1307">#1307</a></li>
</ul>
<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1287">#1287</a>] Updated Tika Text Detection Component to break up large
  chunks of text. The component now generates tracks with both a <code>PAGE_NUM</code> property and <code>SECTION_NUM</code> property. Please
  refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/java/TikaTextDetection/README.md#overview">README</a>.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1313">#1313</a>] Incorrect JSON output object timestamps for variable frame
  rate videos</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1317">#1317</a>] Tika Text Detection component generates first PDF track
  at <code>PAGE_NUM</code> 2</li>
</ul>
<h2>6.1.5: April 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1300">#1300</a>] Parallelized S3 artifact upload. Use
  the <code>detection.artifact.extraction.parallel.upload.count</code> system property to configure the number of parallel uploads.</li>
</ul>
<h2>6.1.4: April 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1299">#1299</a>] Improved artifact extraction performance when there is no
  rotation or flip</li>
</ul>
<h2>6.1.3: April 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1295">#1295</a>] Improved artifact extraction and markup JNI memory
  utilization</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1297">#1297</a>] Limited Workflow Manager IO threads to a reasonable number</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1296">#1296</a>] Fixed ActiveMQ job priorities</li>
</ul>
<h2>6.1.2: April 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1294">#1294</a>] Limited ffmpeg threads to a reasonable number</li>
</ul>
<h2>6.1.1: April 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1292">#1292</a>] Don't skip artifact extraction for failed media</li>
</ul>
<h2>6.1.0: April 2021</h2>

<h3>OpenMPF Command Line Runner</h3>

<ul>
<li>The Command Line Runner allows users to run jobs with a single component without the Workflow Manager.</li>
<li>It outputs results in a JSON structure that is a subset of the regular OpenMPF output.</li>
<li>It only supports C++ and Python components.</li>
<li>See the
  <a href="https://github.com/openmpf/openmpf-docker/blob/master/CLI_RUNNER.md">README</a>
  for more information.</li>
</ul>
<h3>C++ Batch Component API</h3>

<ul>
<li>Component code should no longer configure Log4CXX. The component executor now handles configuring Log4CXX. Component
  code should call <code>log4cxx::Logger::getLogger("&lt;component-name&gt;")</code>
  to get access to the logger. Calls to <code>log4cxx::xml::DOMConfigurator::configure(logconfig_file);</code>
  should be removed.</li>
</ul>
<h3>Python Batch Component API </h3>

<ul>
<li>Component code should no longer configure logging. The component executor now handles configuring logging. Calls
  to <code>mpf.configure_logging</code> should be replaced with
  <code>logging.getLogger('&lt;component-name&gt;')</code>.</li>
</ul>
<h3>Docker Component Base Images</h3>

<ul>
<li>
<p>In order to support running a component through the CLI runner, C++ component developers should set
  the <code>LD_LIBRARY_PATH</code> environment variable in the final stage of their Dockerfiles. It should generally be set
  like: <code>ENV LD_LIBRARY_PATH $PLUGINS_DIR/&lt;component-name&gt;/lib</code>.</p>
</li>
<li>
<p>Because of the logging changes mentioned above, components no longer need to set the
  <code>COMPONENT_LOG_NAME</code> environment variable in their Dockerfiles.</p>
</li>
<li>
<p>Added the
  <a href="https://github.com/openmpf/openmpf-docker/blob/master/components/python/README.md#openmpf_python_executor_ssb"><code>openmpf_python_executor_ssb</code> base image</a>
  . It can be used instead of <code>openmpf_python_component_build</code> and <code>openmpf_python_executor</code> to simplify Dockerfiles for
  Python components that are pure Python and have no build time dependencies.</p>
</li>
</ul>
<h3>Label Moving vs. Non-Moving Tracks</h3>

<ul>
<li>The Workflow Manager can now identify whether a track is moving or non-moving. This is determined by calculating the
  average bounding box for a track by averaging the size and position of all the detections in the track. Then, for each
  detection in the track, the intersection over union (IoU) is calculated between that detection and the average
  detection. If the IoU for at least <code>MOVING_TRACK_MIN_DETECTIONS</code> number of detections is less than or equal to
  <code>MOVING_TRACK_MAX_IOU</code>, then the track is considered a moving track.</li>
<li>Added the following Workflow Manager job properties. These can be set for any video job:<ul>
<li><code>MOVING_TRACK_LABELS_ENABLED</code>: When set to true, attempt to label tracks as either moving or non-moving objects.
  Each track will have a <code>MOVING</code> property set to <code>TRUE</code> or <code>FALSE</code>.</li>
<li><code>MOVING_TRACKS_ONLY</code>: When set to true, remove any tracks that were marked as not moving.</li>
<li><code>MOVING_TRACK_MAX_IOU</code>: The maximum IoU overlap between detection bounding boxes and the average per-track
  bounding box for objects to be considered moving. Value is expected to be between 0 and 1. Note that the lower
  IoU, the more likely the object is moving.</li>
<li><code>MOVING_TRACK_MIN_DETECTIONS</code>: The minimum number of moving detections for a track to be labeled as moving.</li>
</ul>
</li>
</ul>
<h3>Markup Improvements</h3>

<ul>
<li>Users can now watch videos directly in the OpenMPF web UI within the media pop-up dialog for each job. Most modern web
  browsers support videos encoded in VP9 and H.264. If a video cannot be played, users have the option to download it
  and play it using a stand-alone media player.</li>
<li>To set the markup encoder use <code>MARKUP_VIDEO_ENCODER</code>. The default encoder has changed from <code>mjpeg</code> to <code>vp9</code>. As a
  result, it will take longer to generate marked up videos, but they will be higher quality and can be viewed in the web
  UI.</li>
<li>Each bounding box in the marked up media is now labeled. By default, the label shows the track-level <code>CLASSIFICATION</code>
  and associated confidence value. The information shown in the label can be changed by
  setting <code>MARKUP_LABELS_TEXT_PROP_TO_SHOW</code> and <code>MARKUP_LABELS_NUMERIC_PROP_TO_SHOW</code>. To show information for each
  individual detection, rather than the entire track, set <code>MARKUP_LABELS_FROM_DETECTIONS=TRUE</code>.</li>
<li>Exemplar detections in video tracks include a star icon in their label.</li>
<li>Optionally, set <code>MARKUP_VIDEO_MOVING_OBJECT_ICONS_ENABLED=TRUE</code> to show icons that represent if the track is moving or
  non-moving.</li>
<li>Optionally, set <code>MARKUP_VIDEO_BOX_SOURCE_ICONS_ENABLED=TRUE</code> to show icons that represent the source of the detection.
  For example, if the box is the result of an algorithm detection, tracking performing gap fill, or Workflow Manager
  animation.</li>
<li>Each frame of a marked-up video now has a frame number in the upper right corner.</li>
<li>Please refer to the <a href="../Markup-Guide/index.html">Markup Guide</a> for the complete set of markup properties, icon definitions, and
  encoder considerations.</li>
</ul>
<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1181">#1181</a>] Updated the Tesseract OCR Text Detection component from
  Tesseract version 4.0.0 to 4.1.1</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1232">#1232</a>] Updated the Azure Speech Detection component from Azure
  Batch Transcription version 2.0 to 3.0</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1187">#1187</a>] EXIF orientation is now preserved during markup and artifact
  extraction</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1257">#1257</a>] Updated <code>OUTPUT_LAST_TASK_ONLY</code> to work on all media types</li>
</ul>
<h1 id="openmpf-60x">OpenMPF 6.0.x</h1>
<h2>6.0.11: March 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1284">#1284</a>] Updated the Azure Translation component to count emoji as 2
  characters</li>
</ul>
<h2>6.0.10: March 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1270">#1270</a>] The Azure Cognitive Services components now retry HTTP
  requests</li>
</ul>
<h2>6.0.9: March 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1273">#1273</a>] Setting <code>TRANSLATION</code> to the empty string no longer prevents
  Keyword Tagging</li>
</ul>
<h2>6.0.6: March 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1265">#1265</a>] Updated the Tika Text Detection component to handle
  spreadsheets</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1268">#1268</a>] Updated the Tika Text Detection component to remove metadata</li>
</ul>
<h2>6.0.5: February 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1266">#1266</a>] The Azure Translation component now handles the final
  segment correctly when guessing sentence breaks</li>
</ul>
<h2>6.0.4: February 2021</h2>

<h3>Updates</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1264">#1264</a>] Updated the Azure Translation component to handle large
  amounts of text</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1269">#1269</a>] AzureTranslation no longer tries to translate text that is
  already in the <code>TO_LANGUAGE</code></li>
</ul>
<h2>6.0.3: February 2021</h2>

<h3>OpenCV YOLO Detection Component</h3>

<ul>
<li>This new component utilizes the OpenCV Deep Neural Networks (DNN) framework to detect and classify objects in images
  and videos using Darknet YOLOv4 models trained on the COCO dataset. It supports both CPU and GPU modes of operation.
  Tracking is performed using a combination of intersection over union, pixel difference after Fast Fourier transform (
  FFT) phase correlation, Kalman filtering, and OpenCV MOSSE tracking. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/tree/master/cpp/OcvYoloDetection#readme">README</a> for details.</li>
</ul>
<h2>6.0.2: January 2021</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1249">#1249</a>] FFmpeg no longer reports different frame counts for the same
  piece of media</li>
</ul>
<h2>6.0.1: December 2020</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1238">#1238</a>] The JSON output object is now generated when remote media
  cannot be downloaded.</li>
</ul>
<h2>6.0.0: December 2020</h2>

<h3>Upgrade to OpenCV 4.5.0</h3>

<ul>
<li>Updated core framework and components from OpenCV 3.4.7 to OpenCV 4.5.0.</li>
<li>OpenCV is now built with CUDA support, including cuDNN (CUDA Deep Neural Network library) and cuBLAS (CUDA Basic
  Linear Algebra Subroutines library). All C++ components that use the base C++ builder and executor Docker images have
  CUDA support built in, giving developers the option to make use of it.</li>
<li>Added GPU support to the OcvDnnDetection component.</li>
</ul>
<h3>Azure Cognitive Services (ACS) Translation Component</h3>

<ul>
<li>This new component utilizes
  the <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/translator/reference/v3-0-translate">Azure Cognitive Services Translator REST endpoint</a>
  to translate text from one language (locale) to another. Generally, it's intended to operate on feed-forward tracks
  that contain detections with <code>TEXT</code> and <code>TRANSCRIPT</code> properties. It can also operate on plain text file inputs. Refer
  to the <a href="https://github.com/openmpf/openmpf-components/blob/master/python/AzureTranslation/README.md">README</a> for
  details.</li>
</ul>
<h3>Interoperability Package</h3>

<ul>
<li>Added <code>algorithm</code> field to the element that describes a collection of tracks generated by an action in the JSON output
  object. For example:</li>
</ul>
<pre><code>&quot;output&quot;: {
  &quot;FACE&quot;: [{
    &quot;source&quot;: &quot;+#MOG MOTION DETECTION PREPROCESSOR ACTION#OCV FACE DETECTION ACTION&quot;,
    &quot;algorithm&quot;: &quot;FACECV&quot;,
    &quot;tracks&quot;: [{ ... }],
    ...
   },
</code></pre>
<h3>Merge Tasks in JSON Output Object</h3>

<ul>
<li>The output of two tasks in the JSON output object can be merged by setting the <code>OUTPUT_MERGE_WITH_PREVIOUS_TASK</code>
  property to true. This is a Workflow Manager property and can be set on any task in any pipeline, although it has no
  effect when set on the first task or the Markup task.</li>
<li>When the output of two tasks are merged, the tracks for the previous task will not be shown in the JSON output object,
  and no artifacts are generated for it. The task will be listed under <code>TRACKS MERGED</code>, if it's not already listed
  under <code>TRACKS SUPPRESSED</code> due to the <code>mpf.output.objects.last.task.only</code> system property setting,
  or <code>OUTPUT_LAST_TASK_ONLY</code> property. The tracks associated with the second task will inherit the detection type and
  algorithm of the previous task.</li>
<li>For example, the <code>TESSERACT OCR TEXT DETECTION WITH KEYWORD TAGGING PIPELINE</code> is defined as
  the <code>TESSERACT OCR TEXT DETECTION TASK</code> followed by the <code>KEYWORD TAGGING (WITH FF REGION) TASK</code>. The second task
  sets <code>OUTPUT_MERGE_WITH_PREVIOUS_TASK</code> to true. The resulting JSON output object contains one set of keyword-tagged
  OCR tracks that have the <code>TEXT</code> detection type and <code>TESSERACTOCR</code> algorithm (both inherited from
  the <code>TESSERACT OCR TEXT DETECTION TASK</code>):</li>
</ul>
<pre><code>&quot;output&quot;: {
  &quot;TRACKS MERGED&quot;: [{
    &quot;source&quot;: &quot;+#TESSERACT OCR TEXT DETECTION ACTION&quot;,
    &quot;algorithm&quot;: &quot;TESSERACTOCR&quot;
  }],
  &quot;TEXT&quot;: [{
    &quot;source&quot;: &quot;+#TESSERACT OCR TEXT DETECTION ACTION#KEYWORD TAGGING (WITH FF REGION) ACTION&quot;,
    &quot;algorithm&quot;: &quot;TESSERACTOCR&quot;,
    &quot;tracks&quot;: [{
      &quot;type&quot;: &quot;TEXT&quot;,
      &quot;trackProperties&quot;: {
         &quot;TAGS&quot;: &quot;ANIMAL&quot;,
         &quot;TEXT&quot;: &quot;The quick brown fox&quot;,
         &quot;TEXT_LANGUAGE&quot;: &quot;script/Latin&quot;,
         &quot;TRIGGER_WORDS&quot;: &quot;fox&quot;,
         &quot;TRIGGER_WORDS_OFFSET&quot;: &quot;16-18&quot;
         ...
</code></pre>
<ul>
<li>Note that you can use the <code>OUTPUT_MERGE_WITH_PREVIOUS_TASK</code> setting on multiple tasks. For example, if you set it as a
  job property it will be applied to all tasks (with the exception of Markup - in which case the task before Markup is
  used), so you will only get the output of the last task in the pipeline. The last task will inherit the detection type
  and algorithm of the first task in the pipeline.</li>
</ul>
<h3>Tesseract Custom Dictionaries</h3>

<ul>
<li>The Tesseract component Docker image now contains an <code>/opt/mpf/tessdata_model_updater</code> binary that you can use to
  update <code>*.traineddata</code> models with a custom dictionary, as well as extract files from existing models. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/TesseractOCRTextDetection/DICTIONARIES.md">DICTIONARIES</a>
  guide to learn how to use the tool.</li>
<li>In general, legacy <code>*.traineddata</code> models are more influenced by words in their dictionary than more modern
  LSTM <code>*.traineddata</code> models. Also, refer to the known issue below.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1243">#1243</a>] Unpacking a <code>*.traineddata</code> model, for example, in order to
  modify its dictionary, and then repacking it may result in dropping some of the words present in the original
  dictionary file. This may be due to some kind of compression or filtering. It's unknown what effect this has on OCR
  results.</li>
</ul>
<h1 id="openmpf-51x">OpenMPF 5.1.x</h1>
<h2>5.1.3: December 2020</h2>

<h3>Setting Properties as Docker Environment Variables</h3>

<ul>
<li>Any property that can be set as a job property can now be set as a Docker environment variable by prefixing it
  with <code>MPF_PROP_</code>. For example, setting the <code>MPF_PROP_TRTIS_SERVER</code> environment variable in the <code>trtis-detection</code>
  service in your <code>docker-compose.yml</code> file will have the same effect as setting the <code>TRTIS_SERVER</code> job property.</li>
<li>Properties set in this way will take precedence over all other property types (job, algorithm, media, etc). It is not
  possible to change the value of properties set via environment variables at runtime and therefore they should only be
  used to specify properties that will not change throughout the entire lifetime of the service.</li>
</ul>
<h3>Updates</h3>

<ul>
<li>The <code>mpf.output.objects.censored.properties</code> system property can be used to prevent properties from being shown in
  JSON output objects. The value for these properties will appear as <code>&lt;censored&gt;</code>.</li>
<li>The Azure Speech Detection component now retries without diarization when diarization is not supported by the selected
  locale.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1230">#1230</a>] The Azure Speech Detection component now uses a UUID for the
  recording id associated with a piece of media in order to prevent deleting a piece of media while it's in use.</li>
</ul>
<h2>5.1.1: December 2020</h2>

<h3>Updates</h3>

<ul>
<li>Only generate <code>FRAME_COUNT</code> warning when the frame difference is &gt; 1. This can be configured using
  the <code>warn.frame.count.diff</code> system property.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1209">#1209</a>] The Keyword Tagging component now generates video tracks in
  the JSON output object.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1212">#1212</a>] The Keyword Tagging component now preserves the detection
  bounding box and confidence.</li>
</ul>
<h2>5.1.0: November 2020</h2>

<h3>Media Inspection Improvements</h3>

<ul>
<li>The Workflow Manager will now handle video files that don't have a video stream as an <code>AUDIO</code> type, and handle video
  files that don't have a video or audio stream as an <code>UNKNOWN</code> type. The JSON output object contains a
  new <code>media.mediaType</code> field that will be set to <code>VIDEO</code>, <code>AUDIO</code>, <code>IMAGE</code>, or <code>UNKNOWN</code>.</li>
<li>The Workflow Manager now configures Tika
  with <a href="https://github.com/openmpf/openmpf/blob/master/trunk/workflow-manager/src/main/resources/org/apache/tika/mime/custom-mimetypes.xml">custom MIME type support</a>
  . Currently, this enables the detection of <code>video/vnd.dlna.mpeg-tts</code> and <code>image/jxr</code> MIME types.</li>
<li>If the Workflow Manager cannot use Tika to determine the media MIME type then it will fall back to using the
  Linux <code>file</code> command with
  a <a href="https://github.com/openmpf/openmpf/blob/master/trunk/workflow-manager/src/main/resources/magic/custom-magic">custom magicfile</a>
  .</li>
<li>OpenMPF now supports Apple-optimized PNGs and HEIC images. Refer to the Bug Fixes section below.</li>
</ul>
<h3>EAST Text Region Detection Component Improvements</h3>

<ul>
<li>The <code>TEMPORARY_PADDING</code> property has been separated into <code>TEMPORARY_PADDING_X</code> and <code>TEMPORARY_PADDING_Y</code> so that X and
  Y padding can be configured independently.</li>
<li>The <code>MERGE_MIN_OVERLAP</code> property has been renamed to <code>MERGE_OVERLAP_THRESHOLD</code> so that setting it to a value of 0 will
  merge all regions that touch, regardless of how small the amount of overlap.</li>
<li>Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/python/EastTextDetection/README.md#properties">README</a>
  for details.</li>
</ul>
<h3>MPFVideoCapture and MPFImageReader Tool Improvements</h3>

<ul>
<li>These tools now support a <code>ROTATION_FILL_COLOR</code> property for setting the fill color for pixels near the corners and
  edges of frames when performing non-orthogonal rotations. Previously, the color was hardcoded to <code>BLACK</code>. That is
  still the default setting for most components. Now the color can be set to <code>WHITE</code>, which is the default setting for
  the Tesseract component.</li>
<li>These tools now support a <code>ROTATION_THRESHOLD</code> property for adjusting the threshold at which the frame transformer
  performs rotation. Previously, the value was hardcoded to 0.1 degrees. That is still the default value. Rotation is
  not performed on any <code>ROTATION</code> value less than that threshold. The motivation is that some algorithms detect small
  rotations (for example, on structured text) when there is no rotation. In such cases rotating the frame results in
  fewer detections.</li>
<li>OpenMPF now uses FFmpeg when counting video frames. Refer to the Bug Fixes section below.</li>
</ul>
<h3>Azure Cognitive Services (ACS) Form Detection Component</h3>

<ul>
<li>This new component utilizes
  the <a href="https://westus2.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2/operations/AnalyzeLayoutAsync">Azure Cognitive Services Form Detection REST endpoint</a>
  to extract formatted text from documents (PDFs) and images. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/python/AzureFormDetection/README.md">README</a> for
  details.</li>
<li>This component is capable of performing detections using a specified ACS endpoint URL. For example, different
  endpoints support receipt detection, business card detection, layout analysis, and support for custom models trained
  with or without labeled data.</li>
<li>This component may output the following detection properties depending on the endpoint, model, and media being
  processed: <code>TEXT</code>, <code>TABLE_CSV_OUTPUT</code>, <code>KEY_VALUE_PAIRS_JSON</code>, and <code>DOCUMENT_JSON_FIELDS</code>.</li>
</ul>
<h3>Keyword Tagging Component</h3>

<ul>
<li>This new component performs the same keyword tagging behavior that was previously part of the Tesseract component, but
  does so on feed-forward tracks that generate detections with <code>TEXT</code> and <code>TRANSCRIPT</code> properties. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/KeywordTagging/README.md">README</a> for details.</li>
<li>In addition to the Tesseract component, keyword tagging behavior has been removed from the Tika Text component and ACS
  OCR component.</li>
<li>Example pipelines have been added to the following components which make use of a final Keyword Tagging component
  stage:<ul>
<li>Tesseract</li>
<li>Tika Text</li>
<li>ACS OCR</li>
<li>Sphinx</li>
<li>ACS Speech</li>
</ul>
</li>
</ul>
<h3>Optionally Skip Media Inspection</h3>

<ul>
<li>The Workflow Manager will skip media inspection if all of the required media metadata is provided in the job request.
  The <code>MEDIA_HASH</code> and <code>MIME_TYPE</code> fields are always required. Depending on the media data type, other fields may be
  required or optional:<ul>
<li>Images<ul>
<li>Required: <code>FRAME_WIDTH</code>, <code>FRAME_HEIGHT</code></li>
<li>Optional: <code>HORIZONTAL_FLIP</code>, <code>ROTATION</code></li>
</ul>
</li>
<li>Videos<ul>
<li>Required: <code>FRAME_WIDTH</code>, <code>FRAME_HEIGHT</code>, <code>FRAME_COUNT</code>, <code>FPS</code>, <code>DURATION</code></li>
<li>Optional: <code>HORIZONTAL_FLIP</code>, <code>ROTATION</code></li>
</ul>
</li>
<li>Audio files<ul>
<li>Required: <code>DURATION</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Updates</h3>

<ul>
<li>Update OpenMPF Python SDK exception handling for Python 3. Now instead of raising an <code>EnvironmentError</code>, which has
  been deprecated in Python 3, the SDK will raise an <code>mpf.DetectionError</code> or allow the underlying exception to be
  thrown.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1028">#1028</a>] OpenMPF can now properly handle Apple-optimized PNGs, which
  have a non-standard data chunk named CgBI before the IHDR chunk. The Workflow Manager
  uses <a href="http://www.jongware.com/pngdefry.html">pngdefry</a> to convert the image into a standard PNG for processing. Before
  this fix, Tika would throw an error when trying to determine the MIME type of the Apple-optimized PNG.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1130">#1130</a>] OpenMPF can now properly handle HEIC images. The Workflow
  Manager uses <a href="https://github.com/strukturag/libheif">libheif</a> to convert the image into a standard PNG for processing.
  Before this fix, the HEIC image was sometimes falsely identified as a video and the Workflow Manager would fail to
  count the number of frames.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1171">#1171</a>] The MIME type in the JSON output object is no longer null
  when there is a frame counting exception.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1192">#1192</a>] When processing videos, the frame count is now obtained from
  both OpenCV and FFmpeg. The lower of the two is used. If they don't match, a <code>FRAME_COUNT</code> warning is generated.
  Before this fix, on some videos OpenCV would return frame counts that were magnitudes higher than the frames that
  could actually be read. This resulted in failing to process many video segments with a  <code>BAD_FRAME_SIZE</code> error.</li>
</ul>
<h1 id="openmpf-50x">OpenMPF 5.0.x</h1>
<h2>5.0.9: October 2020</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1200">#1200</a>] The MPFVideoCapture and MPFImageReader tools now properly
  handle cropping to frame regions when the region coordinates fall outside of the frame boundary. There was a bug that
  would result in an OpenCV error. Note that the bug only occurred when cropping was not performed with rotation or
  flipping.</li>
</ul>
<h2>5.0.8: October 2020</h2>

<h3>Updates</h3>

<ul>
<li>The Tesseract component now supports a <code>TESSDATA_MODELS_SUBDIRECTORY</code> property. The component will look for tessdata
  files in <code>&lt;MODELS_DIR_PATH&gt;/&lt;TESSDATA_MODELS_SUBDIRECTORY&gt;</code>. This allows users to easily switch between <code>tessdata</code>
  , <code>tessdata_best</code>, and <code>tessdata_fast</code> subdirectories.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1199">#1199</a>] Added missing synchronized to InProgressBatchJobsService,
  which was resulting in some jobs staying <code>IN_PROGRESS</code> indefinitely.</li>
</ul>
<h2>5.0.7: September 2020</h2>

<h3>TensorRT Inference Server (TRTIS) Object Detection Component</h3>

<ul>
<li>This new component detects objects in images and videos by making use of
  an <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/">NVIDIA TensorRT Inference Server</a> (
  TRTIS), and calculates features that can later be used by other systems to recognize the same object in other media.
  We provide support for running the server as a separate service during a Docker deployment, but an external server
  instance can be used instead.</li>
<li>By default, the ip_irv2_coco model is supported and will optionally classify detected objects
  using <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/TrtisDetection/plugin-files/models/ip_irv2_coco/ip_irv2_coco.labels">COCO labels</a>
  . Additionally, features can be generated for whole frames, automatically-detected object regions, and user-specified
  regions. Refer to the <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/TrtisDetection/README.md">README</a>
  .</li>
</ul>
<h2>5.0.6: August 2020</h2>

<h3>Enable OcvDnnDetection to Annotate Feed-forward Detections</h3>

<ul>
<li>The OcvDnnDetection component can now by configured to operate only on certain feed-forward detections and annotate
  them with supplementary information. For example, the following pipeline can be configured to generate detections that
  have both <code>CLASSIFICATION</code> and <code>COLOR</code> detection properties:</li>
</ul>
<pre><code>DarknetDetection (person + vehicle) --&gt; OcvDnnDetection (vehicle color)
</code></pre>
<ul>
<li>For example:</li>
</ul>
<pre><code>  &quot;detectionProperties&quot;: {
    &quot;CLASSIFICATION&quot;: &quot;car&quot;,
    &quot;CLASSIFICATION CONFIDENCE LIST&quot;: &quot;0.397336&quot;,
    &quot;CLASSIFICATION LIST&quot;: &quot;car&quot;,
    &quot;COLOR&quot;: &quot;blue&quot;,
    &quot;COLOR CONFIDENCE LIST&quot;: &quot;0.93507; 0.055744&quot;,
    &quot;COLOR LIST&quot;: &quot;blue; gray&quot;
  }
</code></pre>
<ul>
<li>The OcvDnnDetection component now supports the following properties:<ul>
<li><code>CLASSIFICATION_TYPE</code>: Set this value to change the <code>CLASSIFICATION*</code> part of each output property name to
  something else. For example, setting it to <code>COLOR</code> will generate <code>COLOR</code>, <code>COLOR LIST</code>,
  and <code>COLOR CONFIDENCE LIST</code>. When handling feed-foward detections, the pre-existing <code>CLASSIFICATION*</code> properties
  will be carried over and the <code>COLOR*</code> properties will be added to the detection.</li>
<li><code>FEED_FORWARD_WHITELIST_FILE</code>: When <code>FEED_FORWARD_TYPE</code> is provided and not set to <code>NONE</code>, only feed-forward
  detections with class names contained in the specified file will be processed. For, example, a file with only "
  car" in it will result in performing the exclude behavior (below) for all feed-foward detections that do not have
  a <code>CLASSIFICATION</code> of "car".</li>
<li><code>FEED_FORWARD_EXCLUDE_BEHAVIOR</code>: Specifies what to do when excluding detections not specified in
  the <code>FEED_FORWARD_WHITELIST_FILE</code>. Acceptable values are:<ul>
<li><code>PASS_THROUGH</code>: Return the excluded detections, without modification, along with any annotated detections.</li>
<li><code>DROP</code>: Don't return the excluded detections. Only return annotated detections.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Updates</h3>

<ul>
<li>Make interop package work with Java 8 to better support exernal job producers and consumers.</li>
</ul>
<h2>5.0.5: August 2020</h2>

<h3>Updates</h3>

<ul>
<li>Configure Camel not to auto-acknowledge messages. Users can now see the number of pending messages in the ActiveMQ
  management console for queues consumed by the Workflow Manager.</li>
<li>Improve Tesseract OSD fallback behavior. This prevents selecting the OSD rotation from the fallback pass without the
  OSD script from the fallback pass.</li>
</ul>
<h2>5.0.4: August 2020</h2>

<h3>Updates</h3>

<ul>
<li>Retry job callbacks when they fail. The Workflow Manager now supports the <code>http.callback.timeout.ms</code>
  and <code>http.callback.retries</code> system properties.</li>
<li>Drop "duplicate paged in from cursor" DLQ messages.</li>
</ul>
<h2>5.0.3: July 2020</h2>

<h3>Updates</h3>

<ul>
<li>Update ActiveMQ to 5.16.0.</li>
</ul>
<h2>5.0.2: July 2020</h2>

<h3>Updates</h3>

<ul>
<li>Disable video segmentation for ACS Speech Detection to prevent issues when generating speaker ids.</li>
</ul>
<h2>5.0.1: July 2020</h2>

<h3>Updates</h3>

<ul>
<li>Updated Tessseract component with <code>MAX_PIXELS</code> setting to prevent processing large images.</li>
</ul>
<h2>5.0.0: June 2020</h2>

<h3>Documentation</h3>

<ul>
<li>Updated the openmpf-docker repo <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md">README</a>
  and <a href="https://github.com/openmpf/openmpf-docker/blob/master/SWARM.md">SWARM</a> guides to describe the new build process,
  which now includes automatically copying the openmpf repo source code into the openmpf-build image instead of using
  various bind mounts, and building all of the component base builder and executor images.</li>
<li>Updated the openmpf-docker repo <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md">README</a> with the
  following sections:<ul>
<li>How
  to <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md#optional-use-kibana-for-log-viewing-and-aggregation">Use Kibana for Log Viewing and Aggregation</a></li>
<li>How
  to <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md#optional-restrict-media-types-that-a-component-can-process">Restrict Media Types that a Component Can Process</a></li>
<li>How
  to <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md#optional-import-root-certificates-for-additional-certificate-authorities">Import Root Certificates for Additional Certificate Authorities</a></li>
</ul>
</li>
<li>Updated the <a href="https://github.com/openmpf/openmpf-docker/blob/master/CONTRIBUTING.md">CONTRIBUTING</a> guide for Docker
  deployment with information on the new build process and component base builder and executor images.</li>
<li>Updated the <a href="../Install-Guide/index.html">Install Guide</a> with a pointer to the "Quick Start" section on DockerHub.</li>
<li>Updated the <a href="../REST-API/index.html">REST API</a> with the new endpoints for getting, deleting, and creating actions, tasks, and
  pipelines, as well as a change to the <code>[GET] /rest/info</code> endpoint.</li>
<li>Updated the <a href="../CPP-Batch-Component-API/index.html">C++ Batch Component API</a> to describe changes to the <code>GetDetection()</code> calls,
  which now return a collection of detections or tracks instead of an error code, and to describe improvements to
  exception handling.</li>
<li>Updated the <a href="../CPP-Batch-Component-API/index.html">C++ Batch Component API</a>
  , <a href="../Python-Batch-Component-API/index.html">Python Batch Component API</a>,
  and <a href="../Java-Batch-Component-API/index.html">Java Batch Component API</a> with <code>MIME_TYPE</code>, <code>FRAME_WIDTH</code>, and <code>FRAME_HEIGHT</code> media
  properties.</li>
<li>Updated the <a href="../Python-Batch-Component-API/index.html">Python Batch Component API</a> with information on Python3 and the
  simplification of using a <code>dict</code> for some of the data members.</li>
</ul>
<h3>JSON Output Object</h3>

<ul>
<li>Renamed <code>stages</code> to <code>tasks</code> for clarity and consistency with the rest of the code.</li>
<li>The <code>media</code> element no longer contains a <code>message</code> field.</li>
<li>Each <code>detectionProcessingError</code> element now contains a <code>code</code> field.</li>
<li>Errors and warnings are now grouped by <code>mediaId</code> and summarized using a <code>details</code> element that contains a <code>source</code>
  , <code>code</code>, and <code>message</code> field. Refer
  to <a href="https://github.com/openmpf/openmpf/issues/780#issuecomment-641295884">this comment</a> for an example of the JSON
  structure. Note that errors and warnings generated by the Workflow Manager do not have a <code>mediaId</code>.<ul>
<li>When an error or warning occurs in multiple frames of a video for a single piece of media it will be represented
  in one <code>details</code> element and the <code>message</code> will list the frame ranges.</li>
</ul>
</li>
</ul>
<h3>Interoperability Package</h3>

<ul>
<li>Renamed <code>JsonStage.java</code> to <code>JsonTask.java</code>.</li>
<li>Removed <code>JsonJobRequest.java</code>.</li>
<li>Modified <code>JsonDetectionProcessingError.java</code> by removing the <code>startOffset</code> and <code>stopOffset</code> fields and adding the
  following new fields: <code>startOffsetFrame</code>, <code>stopOffsetFrame</code>, <code>startOffsetTime</code>, <code>stopOffsetTime</code>, and <code>code</code>.</li>
<li>Updated <code>JsonMediaOutputObject.java</code> by removing <code>message</code> field.</li>
<li>Added <code>JsonMediaIssue.java</code> and <code>JsonIssueDetails.java</code>.</li>
</ul>
<h3>Persistent Database</h3>

<ul>
<li>The <code>input_object</code> column in the <code>job_request</code> table has been renamed to <code>job</code> and the content now contains a
  serialized form of <code>BatchJob.java</code> instead of <code>JsonJobRequest.java</code>.</li>
</ul>
<h3>C++ Batch Component API</h3>

<ul>
<li>The <code>GetDetection()</code> calls now return a collection instead of an error code:<ul>
<li><code>std::vector&lt;MPFImageLocation&gt; GetDetections(const MPFImageJob &amp;job)</code></li>
<li><code>std::vector&lt;MPFVideoTrack&gt; GetDetections(const MPFVideoJob &amp;job)</code></li>
<li><code>std::vector&lt;MPFAudioTrack&gt; GetDetections(const MPFAudioJob &amp;job)</code></li>
<li><code>std::vector&lt;MPFGenericTrack&gt; GetDetections(const MPFGenericJob &amp;job)</code></li>
</ul>
</li>
<li><code>MPFDetectionException</code> can now be constructed with a <code>what</code> parameter representing a descriptive error message:<ul>
<li><code>MPFDetectionException(MPFDetectionError error_code, const std::string &amp;what = "")</code></li>
<li><code>MPFDetectionException(const std::string &amp;what)</code></li>
</ul>
</li>
</ul>
<h3>Python Batch Component API</h3>

<ul>
<li>Simplified the <code>detection_properties</code> and <code>frame_locations</code> data members to use a Python <code>dict</code> instead of a custom
  data type.</li>
</ul>
<h3>Full Docker Conversion</h3>

<ul>
<li>Each component is now encapsulated in its own Docker image which self-registers with the Workflow Manager at runtime.
  This deconflicts component dependencies, and allows for greater flexibility when deciding which components to deploy
  at runtime.</li>
<li>The Node Manager image has been removed. For Docker deployments, component services should be managed using Docker
  tools external to OpenMPF.</li>
<li>In Docker deployments, streaming job REST endpoints are disabled, the Nodes web page is no longer available, component
  tar.gz packages cannot be registered through the Component Registration web page, and the <code>mpf</code> command line script
  can now only be run on the Workflow Manager container to modify user settings. The preexisting features are now
  reserved for non-Docker deployments and development environments.</li>
<li>The OpenMPF Docker stack can optionally be deployed with <a href="https://www.elastic.co/kibana">Kibana</a> (which depends on
  Elasticsearch and Filebeat) for viewing log files. Refer to the
  openmpf-docker <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md#optional-use-kibana-for-log-viewing-and-aggregation">README</a>
  .</li>
</ul>
<h3>Docker Component Base Images</h3>

<ul>
<li>A base builder image and executor image are provided for
  C++ (<a href="https://github.com/openmpf/openmpf-docker/blob/master/components/cpp_executor/README.md">README</a>),
  Python (<a href="https://github.com/openmpf/openmpf-docker/blob/master/components/python_executor/README.md">README</a>), and
  Java (<a href="https://github.com/openmpf/openmpf-docker/blob/master/components/java_executor/README.md">README</a>) component
  development. Component developers can also refer to the Dockerfile in the source code for each component as reference
  for how to make use of the base images.</li>
</ul>
<h3>Restrict Media Types that a Component Can Process</h3>

<ul>
<li>Each component service now supports an optional <code>RESTRICT_MEDIA_TYPES</code> Docker environment variable that specifies the
  types of media that service will process. For example, <code>RESTRICT_MEDIA_TYPES: VIDEO,IMAGE</code> will process both videos
  and images, while <code>RESTRICT_MEDIA_TYPES: IMAGE</code> will only process images. If not specified, the service will process
  all of the media types it natively supports. For example, this feature can be used to ensure that some services are
  always available to process images while others are processing long videos.</li>
</ul>
<h3>Import Additional Root Certificates into the Workflow Manager</h3>

<ul>
<li>Additional root certificates can be imported into the Workflow Manager at runtime by adding an entry
  for <code>MPF_CA_CERTS</code> to the workflow-manager service's environment variables in <code>docker-compose.core.yml</code>
  . <code>MPF_CA_CERTS</code> must contain a colon-delimited list of absolute file paths. Of note, a root certificate may be used
  to trust the identity of a remote object storage server.</li>
</ul>
<h3>DockerHub</h3>

<ul>
<li>Pushed prebuilt OpenMPF Docker images to <a href="https://hub.docker.com/u/openmpf">DockerHub</a>. Refer to the "Quick Start"
  section of the OpenMPF Workflow Manager
  image <a href="https://hub.docker.com/r/openmpf/openmpf_workflow_manager">documentation</a>.</li>
</ul>
<h3>Version Updates</h3>

<ul>
<li>Updated from Oracle Java 8 to OpenJDK 11, which required updating to Tomcat 8.5.41. We now
  use <a href="https://codehaus-cargo.github.io/cargo/Home.html">Cargo</a> to run integration tests.</li>
<li>Updated OpenCV from 3.0.0 to 3.4.7 to update Deep Neural Networks (DNN) support.</li>
<li>Updated Python from 2.7 to 3.8.2.</li>
</ul>
<h3>FFmpeg</h3>

<ul>
<li>We are no longer building separate audio and video encoders and decoders for FFmpeg. Instead, we are using the
  built-in decoders that come with FFmpeg by default. This simplifies the build process and redistribution via Docker
  images.</li>
</ul>
<h3>Artifact Extraction</h3>

<ul>
<li>The  <code>ARTIFACT_EXTRACTION_POLICY</code> property can now be assigned a value of <code>NONE</code>, <code>VISUAL_TYPES_ONLY</code>, <code>ALL_TYPES</code>,
  or <code>ALL_DETECTIONS</code>.<ul>
<li>With the <code>VISUAL_TYPES_ONLY</code> or <code>ALL_TYPES</code> policy, artifacts will be extracted according to
  the <code>ARTIFACT_EXTRACTION_POLICY*</code> properties. With the <code>NONE</code> and <code>ALL_DETECTIONS</code> policies, those settings are
  ignored.</li>
<li>Note that previously <code>NONE</code>, <code>VISUAL_EXEMPLARS_ONLY</code>, <code>EXEMPLARS_ONLY</code>, <code>ALL_VISUAL_DETECTIONS</code>,
  and <code>ALL_DETECTIONS</code> were supported.</li>
</ul>
</li>
<li>The following <code>ARTIFACT_EXTRACTION_POLICY*</code> properties are now supported:<ul>
<li><code>ARTIFACT_EXTRACTION_POLICY_EXEMPLAR_FRAME_PLUS</code>: Extract the exemplar frame from the track, plus this many frames
  before and after the exemplar.</li>
<li><code>ARTIFACT_EXTRACTION_POLICY_FIRST_FRAME</code>: If true, extract the first frame from the track.</li>
<li><code>ARTIFACT_EXTRACTION_POLICY_MIDDLE_FRAME</code>: If true, extract the frame with a detection that is closest to the
  middle frame from the track.</li>
<li><code>ARTIFACT_EXTRACTION_POLICY_LAST_FRAME</code>: If true, extract the last frame from the track.</li>
<li><code>ARTIFACT_EXTRACTION_POLICY_TOP_CONFIDENCE_COUNT</code>: Sort the detections in a track by confidence and then extract
  this many detections, starting with those which have the highest confidence.</li>
<li><code>ARTIFACT_EXTRACTION_POLICY_CROPPING</code>: If true, an artifact will be extracted for each detection in each frame
  that is selected according to the other <code>ARTIFACT_EXTRACTION_POLICY*</code> properties. The extracted artifact will be
  cropped to the width and height of the detection bounding box, and the artifact will be rotated according to the
  detection <code>ROTATION</code> property. If false, the artifact extraction behavior is unchanged from the previous release:
  the entire frame will be extracted without any rotation.</li>
</ul>
</li>
<li>For clarity, <code>OUTPUT_EXEMPLARS_ONLY</code> has been renamed to <code>OUTPUT_ARTIFACTS_AND_EXEMPLARS_ONLY</code>. Extracted artifacts
  will always be reported in the JSON output object.</li>
<li>The <code>mpf.output.objects.exemplars.only</code> system property has been renamed
  to <code>mpf.output.objects.artifacts.and.exemplars.only</code>. It works the same as before with the exception that if an
  artifact is extracted for a detection then that detection will always be represented in the JSON output object,
  whether it's an exemplar or not.</li>
<li>The <code>mpf.output.objects.last.stage.only</code> system property has been renamed to <code>mpf.output.objects.last.task.only</code>. It
  works the same as before with the exception that when set to true artifact extraction is skipped for all tasks but the
  last task.</li>
</ul>
<h3>REST Endpoints</h3>

<ul>
<li>Modified <code>[GET] /rest/info</code>. Now returns output like <code>{"version": "4.1.0", "dockerEnabled": true}</code>.</li>
<li>Added the following REST endpoints for getting, removing, and creating actions, tasks, and pipelines. Refer to
  the <a href="../REST-API/index.html">REST API</a> for more information:<ul>
<li><code>[GET] /rest/actions</code>, <code>[GET] /rest/tasks</code>, <code>[GET] /rest/pipelines</code></li>
<li><code>[DELETE] /rest/actions</code>, <code>[DELETE] /rest/tasks</code>, <code>[DELETE] /rest/pipelines</code></li>
<li><code>[POST] /rest/actions</code> , <code>[POST] /rest/tasks</code>, <code>[POST] /rest/pipelines</code></li>
</ul>
</li>
<li>All of the endpoints above are new with the exception of <code>[GET] /rest/pipelines</code>. The endpoint has changed since the
  last version of OpenMPF. Some fields in the response JSON have been removed and renamed. Also, it now returns a
  collection of tasks for each pipelines. Refer to the REST API.</li>
<li><code>[GET] /rest/algorithms</code> can be used to get information about algorithms. Note that algorithms are tied to registered
  components, so to remove an algorithm you must unregister the associated component. To add an algorithm, start the
  associated component's Docker container so it self-registers with the Workflow Manager.</li>
</ul>
<h3>Incomplete Actions, Tasks, and Pipelines</h3>

<ul>
<li>The previous version of OpenMPF would generate an error when attempting to register a component that included actions,
  tasks, or pipelines that depend on algorithms, actions, or tasks that are not yet registered with the Workflow
  Manager. This required components to be registered in a specific order. Also, when unregistering a component, it
  required the components which depend on it to be unregistered. These dependency checks are no longer enforced.</li>
<li>In general, the Workflow Manager now appropriately handles incomplete actions, tasks, and pipelines by checking if all
  of the elements are defined before executing a job, and then preserving that information in memory until the job is
  complete. This allows components to be registered and removed in an arbitrary order without affecting the state of
  other components, actions, tasks, or pipelines. This also allows actions and tasks to be removed using the new REST
  endpoints and then re-added at a later time while still preserving the elements that depend on them.</li>
<li>Note that unregistering a component while a job is running will cause it to stall. Please ensure that no jobs are
  using a component before unregistering it.</li>
</ul>
<h3>Python Arbitrary Rotation</h3>

<ul>
<li>The Python MPFVideoCapture and MPFImageReader tools now support <code>ROTATION</code> values other than 0, 90, 180, and 270
  degrees. Users can now specify a clockwise <code>ROTATION</code> job property in the range [0, 360). Values outside that range
  will be normalized to that range. Floating point values are accepted. This is similar to the existing support
  for <a href="#cpp-arbitrary-rotation">C++ arbitrary rotation</a>.</li>
</ul>
<h3>OpenCV Deep Neural Networks (DNN) Detection Component</h3>

<ul>
<li>This new component replaces the old CaffeDetection component. It supports the same GoogLeNet and Yahoo Not Suitable
  For Work (NSFW) models as the old component, but removes support for the Rezafuad vehicle color detection model in
  favor of a custom TensorFlow vehicle color detection model. In our tests, the new model has proven to be more
  generalizable and provide more accurate results on never-before-seen test data. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/OcvDnnDetection/README.md">README</a>.</li>
</ul>
<h3>Azure Cognitive Services (ACS) Speech Detection Component</h3>

<ul>
<li>This new component utilizes
  the <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/batch-transcription">Azure Cognitive Services Batch Transcription REST endpoint</a>
  to transcribe speech from audio and video files. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/python/AzureSpeechDetection/README.md">README</a>.</li>
</ul>
<h3>Tesseract OCR Text Detection Component</h3>

<ul>
<li>Text tagging has been simplified to only support regular expression searches. Whole keyword searches are a subset of
  regular expression searches, and are therefore still supported. Also, the <code>text-tags.json</code> file format has been
  updated to allow for specifying case-sensitive regular expression searches.</li>
<li>Additionally, the <code>TRIGGER_WORDS</code> and <code>TRIGGER_WORDS_OFFSET</code> detection properties are now supported, which list the
  OCR'd words that resulted in adding a <code>TAG</code> to the detection, and the character offset of those words within the
  OCR'd <code>TEXT</code>, respectively.</li>
<li>Key changes to tagging output and <code>text-tags.json</code> format are outlined below. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/TesseractOCRTextDetection/README.md#text-tagging">README</a>
  for more information:<ul>
<li>Regex patterns should now be entered in the format <code>{"pattern": "regex_pattern"}</code>. Users can add and toggle
  the <code>"caseSensitive"</code> regex flag for each pattern.<ul>
<li>For example: <code>{"pattern": "(\\b)bus(\\b)", "caseSensitive": true}</code> enables case-sensitive regex pattern
  matching.</li>
<li>By default, each regex pattern, including those in the legacy format, will be case-insensitive.</li>
</ul>
</li>
<li>As part of the text tagging update, the <code>TAGS</code> outputs are now separated by semicolons <code>;</code> rather than commas <code>,</code>
  to be consistent with the delimiters for <code>TRIGGER_WORDS</code> and <code>TRIGGER_WORDS_OFFSET</code> output patterns.<ul>
<li>Because semicolons can be part of the trigger word itself, those semicolons will be encapsulated in brackets.<ul>
<li>For example, <code>detected trigger with a ;</code> in the OCR'd <code>TEXT</code> is reported
  as <code>TRIGGER_WORDS=detected trigger with a [;]; some other trigger</code>.</li>
</ul>
</li>
<li>Commas are now used to group each set of <code>TRIGGER_WORDS_OFFSET</code> with its respective <code>TRIGGER_WORDS</code> output.
  Both <code>TAGS</code> and <code>TRIGGER_WORDS</code> are separated by semicolons only.<ul>
<li>For example: <code>TRIGGER_WORDS=trigger1; trigger2</code>, <code>TRIGGER_WORDS_OFFSET=0-5, 6-10; 12-15</code>, means
  that <code>trigger1</code> occurs twice in the text at the index ranges 0-5 and 6-10, and <code>trigger2</code> occurs at index
  range 12-15.</li>
</ul>
</li>
</ul>
</li>
<li>Regex tagging now follows the C++ ECMAS format (
  see <a href="http://www.cplusplus.com/reference/regex/ECMAScript/">examples here</a>) after resolving JSON string conversion
  for regex tags.</li>
<li>As a result the regex patterns <code>\b</code> and <code>\p</code> in the text tagging file must now be written as <code>\\b</code> and <code>\\p</code>,
  respectively, to match the format of other regex character patterns (ex. <code>\\d</code>, <code>\\w</code>, <code>\\s</code>, etc.).</li>
</ul>
</li>
<li>The <code>MAX_PARALLEL_SCRIPT_THREADS</code> and <code>MAX_PARALLEL_PAGE_THREADS</code> properties are now supported. When processing
  images, the first property is used to determine how many threads to run in parallel. Each thread performs OCR using a
  different language or script model. When processing PDFs, the second property is used to determine how many threads to
  run in parallel. Each thread performs OCR on a different page of the PDF.</li>
<li>The <code>ENABLE_OSD_FALLBACK</code> property is now supported. If enabled, an additional round of OSD is performed when the
  first round fails to generate script predictions that are above the OSD score and confidence thresholds. In the second
  pass, the component will run OSD on multiple copies of the input text image to get an improved prediction score
  and <code>OSD_FALLBACK_OCCURRED</code> detection property will be set to true.</li>
<li>If any OSD-detected models are missing, the new <code>MISSING_LANGUAGE_MODELS</code> detection property will list the missing
  models.</li>
</ul>
<h3>Tika Text Detection Component</h3>

<ul>
<li>The Tika text detection component now supports text tagging in the same way as the Tesseract component. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/java/TikaTextDetection/README.md#text-tagging">README</a>.</li>
</ul>
<h3>Other Improvements</h3>

<ul>
<li>Simplified component <code>descriptor.json</code> files by moving the specification of common properties, such
  as <code>CONFIDENCE_THRESHOLD</code>, <code>FRAME_INTERVAL</code>, <code>MIN_SEGMENT_LENGTH</code>, etc., to a single <code>workflow-properties.json</code> file.
  Now when the Workflow Manager is updated to support new features, the component <code>descriptor.json</code> file will not need
  to be updated.</li>
<li>Updated the Sphinx component to return <code>TRANSCRIPT</code> instead of <code>TRANSCRIPTION</code>, which is grammatically correct.</li>
<li>Whitespace is now trimmed from property names when jobs are submitted via the REST API.</li>
<li>The Darknet Docker image now includes the YOLOv3 model weights.</li>
<li>The C++ and Python ModelsIniParser now allows users to specify optional fields.</li>
<li>When a job completion callback fails, but otherwise the job is successful, the final state of the job will
  be <code>COMPLETE_WITH_WARNINGS</code>.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/772">#772</a>] Can now create a custom pipeline with long action names using
  the Pipelines 2 UI.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/812">#812</a>] Now properly setting the start and stop index for elements in
  the <code>detectionProcessingErrors</code> collection in the JSON output object. Errors reported for each job segment will now
  appear in the collection.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/941">#941</a>] Tesseract component no longer segfaults when handling corrupt
  media.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1005">#1005</a>] Fixed a bug that caused a NullPointerException when
  attempting to get output object JSON via REST before a job completes.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1035">#1035</a>] The search bar in the Job Status UI can once again for used
  to search for job id.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1104">#1104</a>] Fixed C++/Python component executor memory leaks.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1108">#1108</a>] Fixed a bug when handling frames and detections that are
  horizontally flipped. This affected both markup and feed-forward behaviors.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1119">#1119</a>] Fixed Tesseract component memory leaks and uninitialized
  read issues.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1028">#1028</a>] Media inspection fails to handle Apple-optimized PNGs with
  the CgBI data chunk before the IHDR chunk.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1109">#1109</a>] We made the search bar in the Job Status UI more efficient
  by shifting it to a database query, but in doing so introduced a bug where the search operates on UTC time instead of
  local system time.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1010">#1010</a>] <code>mpf.output.objects.enabled</code> does not behave as expected for
  batch jobs. A user would expect it to control whether the JSON output object is generated, but it's generated
  regardless of that setting.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1032">#1032</a>] Jobs fail on corrupt QuickTime videos. For these videos, the
  OpenCV-reported frame count is more than twice the actual frame count.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1106">#1106</a>] When a job ends in ERROR the job status UI does not show an
  End Date.</li>
</ul>
<h1 id="openmpf-41x">OpenMPF 4.1.x</h1>
<h2>4.1.14: June 2020</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1120">#1120</a>] The node-manager Docker image now correctly installs CUDA
  libraries so that GPU-enabled components on that image can run on the GPU.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1064">#1064</a>] Fixed memory leaks in the Darknet component for various
  network types, and when using GPU resources. This bug covers everything not addressed
  by <a href="https://github.com/openmpf/openmpf/issues/1062">#1062</a>.</li>
</ul>
<h2>4.1.13: June 2020</h2>

<h3>Updates</h3>

<ul>
<li>Updated the OpenCV build and media inspection process to properly handle webp images.</li>
</ul>
<h2>4.1.12: May 2020</h2>

<h3>Updates</h3>

<ul>
<li>Updated JDK from <code>jdk-8u181-linux-x64.rpm</code> to <code>jdk-8u251-linux-x64.rpm</code>.</li>
</ul>
<h2>4.1.11: May 2020</h2>

<h3>Tesseract OCR Text Detection Component</h3>

<ul>
<li>Added <code>INVALID_MIN_IMAGE_SIZE</code> job property to filter out images with extremely low width or height.</li>
<li>Updated image rescaling behavior to account for image dimension limits.</li>
<li>Fixed handling of <code>nullptr</code> returns from Tesseract API OCR calls.</li>
</ul>
<h2>4.1.8: May 2020</h2>

<h3>Azure Cognitive Services (ACS) OCR Component</h3>

<ul>
<li>This new component utilizes
  the <a href="https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fc">ACS OCR REST endpoint</a>
  to extract text from images and videos. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/python/AzureOcrTextDetection/README.md">README</a>.</li>
</ul>
<h2>4.1.6: April 2020</h2>

<h3>Updates</h3>

<ul>
<li>Now silently discarding ActiveMQ DLQ "Suppressing duplicate delivery on connection" messages in addition to "duplicate
  from store" messages.</li>
</ul>
<h2>4.1.5: March 2020</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1062">#1062</a>] Fixed a memory leak in the Darknet component that occurred
  when running jobs on CPU resources with the Tiny YOLO model.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1064">#1064</a>] The Darknet component has memory leaks for various network
  types, and potentially when using GPU resources. This bug covers everything not addressed
  by <a href="https://github.com/openmpf/openmpf/issues/1062">#1062</a>.</li>
</ul>
<h2>4.1.4: March 2020</h2>

<h3>Updates</h3>

<ul>
<li>Updated from Hibernate 5.0.8 to 5.4.12 to support schema-based multitenancy. This allows multiple instances of OpenMPF
  to use the same PostgreSQL database as long as each instance connects to the database as a separate user, and the
  database is configured appropriately. This also required updating Tomcat from 7.0.72 to 7.0.76.</li>
</ul>
<h3>JSON Output Object</h3>

<ul>
<li>Updated the Workflow Manager to include an <code>outputobjecturi</code> in GET callbacks, and <code>outputObjectUri</code> in POST
  callbacks, when jobs complete. This URI specifies a file path, or path on the object storage server, depending on
  where the JSON output object is located.</li>
</ul>
<h3>Interoperability Package</h3>

<ul>
<li>Updated <code>JsonCallbackBody.java</code> to contain an <code>outputObjectUri</code> field.</li>
</ul>
<h2>4.1.3: February 2020</h2>

<h3>Features</h3>

<ul>
<li>Added support for <code>DETECTION_PADDING_X</code> and <code>DETECTION_PADDING_Y</code> optional job properties. The value can be a
  percentage or whole-number pixel value. When positive, each detection region in each track will be expanded. When
  negative, the region will shrink. If the detection region is shrunk to nothing, the shrunk dimension(s) will be set to
  a value of 1 pixel and the <code>SHRUNK_TO_NOTHING</code> detection property will be set to true.</li>
<li>Added support for <code>DISTANCE_CONFIDENCE_WEIGHT_FACTOR</code> and <code>SIZE_CONFIDENCE_WEIGHT_FACTOR</code> SuBSENSE algorithm
  properties. Increasing the value of the first property will generate detection confidence values that favor being
  closer to the center frame of a track. Increasing the value of the second property will generate detection confidence
  values that favor large detection regions.</li>
</ul>
<h2>4.1.1: January 2020</h2>

<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/1016">#1016</a>] Fixed a bug that caused a deadlock situation when the media
  inspection process failed quickly when processing many jobs using a pipeline with more than one stage.</li>
</ul>
<h2>4.1.0: July 2019</h2>

<h3>Documentation</h3>

<ul>
<li>Updated the <a href="../CPP-Batch-Component-API/index.html#mpfimagelocation">C++ Batch Component API</a> to describe the <code>ROTATION</code>
  detection property. See the <a href="#cpp-arbitrary-rotation">C++ Arbitrary Rotation</a> section below.</li>
<li>Updated the <a href="../REST-API/index.html">REST API</a> with new component registration REST endpoints. See
  the <a href="#component-registration-rest-endpoints">Component Registration REST Endpoints</a> section below.</li>
<li>Added a <a href="https://github.com/openmpf/openmpf-components/blob/develop/python/EastTextDetection/README.md">README</a> for
  the EAST text region detection component. See
  the <a href="#east-text-region-detection-component">EAST Text Region Detection Component</a> section below.</li>
<li>Updated the Tesseract OCR text detection
  component <a href="https://github.com/openmpf/openmpf-components/blob/develop/cpp/TesseractOCRTextDetection/README.md">README</a>
  . See the  <a href="#tesseract-ocr-text-detection-component">Tesseract OCR Text Detection Component</a> section below.</li>
<li>Updated the openmpf-docker repo <a href="https://github.com/openmpf/openmpf-docker/blob/develop/README.md">README</a>
  and <a href="https://github.com/openmpf/openmpf-docker/blob/develop/SWARM.md">SWARM</a> guide to describe the new streamlined
  approach to using <code>docker-compose config</code>. See the <a href="#docker-deployment">Docker Deployment</a> section below.</li>
<li>Fixed the description of <code>MIN_SEGMENT_LENGTH</code> and associated examples in
  the <a href="../User-Guide/index.html#min_segment_length-property">User Guide</a> for
  issue <a href="https://github.com/openmpf/openmpf/issues/891">#891</a>.</li>
<li>Updated the <a href="../Java-Batch-Component-API/index.html#logging">Java Batch Component API</a> with information on how to use Log4j2.
  Related to resolving issue <a href="https://github.com/openmpf/openmpf/issues/855">#855</a>.</li>
<li>Updated the <a href="../Install-Guide/index.html">Install Guide</a> to point to the
  Docker <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md#getting-started">README</a>.</li>
<li>Transformed the Build Guide into a <a href="../Development-Environment-Guide/index.html">Development Environment Guide</a>.</li>
</ul>
<p><span id="cpp-arbitrary-rotation"></span></p>
<h3>C++ Arbitrary Rotation</h3>

<ul>
<li>The C++ MPFVideoCapture and MPFImageReader tools now support <code>ROTATION</code> values other than 0, 90, 180, and 270 degrees.
  Users can now specify a clockwise <code>ROTATION</code> job property in the range [0, 360). Values outside that range will be
  normalized to that range. Floating point values are accepted.</li>
<li>When using those tools to read frame data, they will automatically correct for rotation so that the returned frame is
  horizontally oriented toward the normal 3 o'clock position.<ul>
<li>When <code>FEED_FORWARD_TYPE=REGION</code>, these tools will look for a <code>ROTATION</code> detection property in the feed-forward
  detections and automatically correct for rotation. For example, a detection property of <code>ROTATION=90</code> represents
  that the region is rotated 90 degrees counter clockwise, and therefore must be rotated 90 degrees clockwise to
  correct for it.</li>
<li>When <code>FEED_FORWARD_TYPE=SUPERSET_REGION</code>, these tools will properly account for the <code>ROTATION</code> detection property
  associated with each feed-forward detection when calculating the bounding box that encapsulates all of those
  regions.</li>
<li>When <code>FEED_FORWARD_TYPE=FRAME</code>, these tools will rotate the frame according to the <code>ROTATION</code> job property. It's
  important to note that for rotations other than 0, 90, 180, and 270 degrees the rotated frame dimensions will be
  larger than the original frame dimensions. This is because the frame needs to be expanded to encapsulate the
  entirety of the original rotated frame region. Black pixels are used to fill the empty space near the edges of the
  original frame.</li>
</ul>
</li>
<li>The Markup component now places a colored dot at the upper-left corner of each detection region so that users can
  determine the rotation of the region relative to the entire frame.</li>
</ul>
<p><span id="component-registration-rest-endpoints"></span></p>
<h3>Component Registration REST Endpoints</h3>

<ul>
<li>Added a <code>[POST] /rest/components/registerUnmanaged</code> endpoint so that components running as separate Docker containers
  can self-register with the Workflow Manager.<ul>
<li>Since these components are not managed by the Node Manager, they are considered unmanaged OpenMPF components.
  These components are not displayed in Nodes web UI and are tagged as unmanaged in the Component Registration web
  UI where they can only be removed.</li>
<li>Note that components uploaded to the Component Registration web UI as .tar.gz files are considered managed
  components.</li>
</ul>
</li>
<li>Added a <code>[DELETE] /rest/components/{componentName}</code> endpoint that can be used to remove managed and unmanaged
  components.</li>
</ul>
<h3>Python Component Executor Docker Image</h3>

<ul>
<li>Component developers can now use a Python component executor Docker image to write a Python component for OpenMPF that
  can be encapsulated within a Docker container. This isolates the build and execution environment from the rest of
  OpenMPF. For more information, see
  the <a href="https://github.com/openmpf/openmpf-docker/blob/develop/openmpf_runtime/python_executor/README.md">README</a>.</li>
<li>Components developed with this image are not managed by the Node Manager; rather, they self-register with the Workflow
  Manager and their lifetime is determined by their own Docker container.</li>
</ul>
<p><span id="docker-deployment"></span></p>
<h3>Docker Deployment</h3>

<ul>
<li>Streamlined single-host <code>docker-compose up</code> deployments and multi-host <code>docker stack deploy</code> swarm deployments. Now
  users are instructed to create a single <code>docker-compose.yml</code> file for both types of deployments.</li>
<li>Removed the <code>docker-generate-compose-files.sh</code> script in favor of allowing users the flexibility of combining
  multiple <code>docker-compose.*.yml</code> files together using <code>docker-compose config</code>. See
  the <a href="https://github.com/openmpf/openmpf-docker/blob/develop/README.md#generate-docker-composeyml">Generate docker-compose.yml</a>
  section of the README.</li>
<li>Components based on the Python component executor Docker image can now be defined and configured directly
  in <code>docker-compose.yml</code>.</li>
<li>OpenMPF Docker images now make use of Docker labels.</li>
</ul>
<p><span id="east-text-region-detection-component"></span></p>
<h3>EAST Text Region Detection Component</h3>

<ul>
<li>This new component uses the Efficient and Accurate Scene Text (EAST) detection model to detect text regions in images
  and videos. It reports their location, angle of rotation, and text type (<code>STRUCTURED</code> or <code>UNSTRUCTURED</code>), and supports
  a variety of settings to control the behavior of merging text regions into larger regions. It does not perform OCR on
  the text or track detections across video frames. Thus, each video track is at most one detection long. For more
  information, see
  the <a href="https://github.com/openmpf/openmpf-components/blob/develop/python/EastTextDetection/README.md">README</a>.</li>
<li>Optionally, this component can be built as a Docker image using the Python component executor Docker image, allowing
  it to exist apart from the Node Manager image.</li>
</ul>
<p><span id="tesseract-ocr-text-detection-component"></span></p>
<h3>Tesseract OCR Text Detection Component</h3>

<ul>
<li>Updated to support reading tessdata <code>*.traineddata</code> files at a specified <code>MODELS_DIR_PATH</code>. This allows users to
  install new <code>*.traineddata</code> files post deployment.</li>
<li>Updated to optionally perform Tesseract Orientation and Script Detection (OSD). When enabled, the component will
  attempt to use the orientation results of OSD to automatically rotate the image, as well as perform OCR using the
  scripts detected by OSD.</li>
<li>Updated to optionally rotate a feed-forward text region 180 degrees to account for upside-down text.</li>
<li>Now supports the following preprocessing properties for both structured and unstructured text:<ul>
<li>Text sharpening</li>
<li>Text rescaling</li>
<li>Otsu image thresholding</li>
<li>Adaptive thresholding</li>
<li>Histogram equalization</li>
<li>Adaptive histogram equalization (also known as Contrast Limited Adaptive Histogram Equalization (CLAHE))</li>
</ul>
</li>
<li>Will use the <code>TEXT_TYPE</code> detection property in feed-forward regions provided by the EAST component to determine which
  preprocessing steps to perform.</li>
<li>For more information on these new features, see
  the <a href="https://github.com/openmpf/openmpf-components/blob/develop/cpp/TesseractOCRTextDetection/README.md">README</a>.</li>
<li>Removed gibberish and string filters since they only worked on English text.</li>
</ul>
<h3>ActiveMQ Profiles</h3>

<ul>
<li>The ActiveMQ Docker image now supports custom profiles. The container selects an <code>activemq.xml</code> and <code>env</code> file to use
  at runtime based on the value of the <code>ACTIVE_MQ_PROFILE</code> environment variable. Among others, these files contain
  configuration settings for Java heap space and component queue memory limits.</li>
<li>This release only supports a <code>default</code> profile setting, as defined by <code>activemq-default.xml</code> and <code>env.default</code>;
  however, developers are free to add other <code>activemq-&lt;profile&gt;.xml</code> and <code>env.&lt;profile&gt;</code> files to the ActiveMQ Docker
  image to suit their needs.</li>
</ul>
<h3>Disabled ActiveMQ Prefetch</h3>

<ul>
<li>Disabled ActiveMQ prefetching on all component queues. Previously, a prefetch value of one was resulting in situations
  where one component service could be dispatched two sub-jobs, thereby starving other available component services
  which could process one of those sub-jobs in parallel.</li>
</ul>
<h3>Search Region Percentages</h3>

<ul>
<li>In addition to using exact pixel values, users can now use percentages for the following properties when specifying
  search regions for C++ and Python components:<ul>
<li><code>SEARCH_REGION_TOP_LEFT_X_DETECTION</code></li>
<li><code>SEARCH_REGION_TOP_LEFT_Y_DETECTION</code></li>
<li><code>SEARCH_REGION_BOTTOM_RIGHT_X_DETECTION</code></li>
<li><code>SEARCH_REGION_BOTTOM_RIGHT_Y_DETECTION</code></li>
</ul>
</li>
<li>For example, setting <code>SEARCH_REGION_TOP_LEFT_X_DETECTION=50%</code> will result in components only processing the right half
  of an image or video.</li>
<li>Optionally, users can specify exact pixel values of some of these properties and percentages for others.</li>
</ul>
<h3>Other Improvements</h3>

<ul>
<li>Increased the number of ActiveMQ maxConcurrentConsumers for the <code>MPF.COMPLETED_DETECTIONS</code> queue from 30 to 60.</li>
<li>The Create Job web UI now only displays the content of the <code>$MPF_HOME/share/remote-media</code> directory instead of all
  of <code>$MPF_HOME/share</code>, which prevents the Workflow Manager from indexing generated JSON output files, artifacts, and
  markup. Indexing the latter resulted in Java heap space issues for large scale production systems. This is a
  mitigation for issue <a href="https://github.com/openmpf/openmpf/issues/897">#897</a>.</li>
<li>The Job Status web UI now makes proper use of pagination in SQL/Hibernate through the Workflow Manager to avoid
  retrieving the entire jobs table, which was inefficient.</li>
<li>The Workflow Manager will now silently discard all duplicate messages in the ActiveMQ Dead Letter Queue (DLQ),
  regardless of destination. Previously, only messages destined for component sub-job request queues were discarded.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/891">#891</a>] Fixed a bug where the Workflow Manager media segmenter
  generated short segments that were minimally <code>MIN_SEGMENT_LENGTH+1</code> in size instead of <code>MIN_SEGMENT_LENGTH</code>.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/745">#745</a>] In environments where thousands of jobs are processed, users
  have observed that, on occasion, pending sub-job messages in ActiveMQ queues are not processed until a new job is
  created. This seems to have been resolved by disabling ActiveMQ prefetch behavior on component queues.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/855">#855</a>] A logback circular reference suppressed exception no longer
  throws a StackOverflowError. This was resolved by transitioning the Workflow Manager and Java components from the
  Logback framework to Log4j2.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/897">#897</a>] OpenMPF will attempt to index files located
  in <code>$MPF_HOME/share</code> as soon as the webapp is started by Tomcat. This is so that those files can be listed in a
  directory tree in the Create Job web UI. The main problem is that once a file gets indexed it's never removed from the
  cache, even if the file is manually deleted, resulting in a memory leak.</li>
</ul>
<h3>Late Additions: November 2019</h3>

<ul>
<li>User names, roles, and passwords can now be set by using an optional <code>user.properties</code> file. This allows
  administrators to override the default OpenMPF users that come preconfigured, which may be a security risk. Refer to
  the "Configure Users" section of the
  openmpf-docker <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md#optional-configure-users">README</a> for
  more information.</li>
</ul>
<h3>Late Additions: December 2019</h3>

<ul>
<li>Transitioned from using a mySQL persistent database to PostgreSQL to support users that use an external PostgreSQL
  database in the cloud.</li>
<li>Updated the EAST component to support a <code>TEMPORARY_PADDING</code> and <code>FINAL_PADDING</code> property. The first property
  determines how much padding is added to detections during the non-maximum suppression or merging step. This padding is
  effectively removed from the final detections. The second property is used to control the final amount of padding on
  the output regions. Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/python/EastTextDetection/README.md#properties">README</a>.</li>
</ul>
<h1 id="openmpf-40x">OpenMPF 4.0.x</h1>
<h2>4.0.0: February 2019</h2>

<h3>Documentation</h3>

<ul>
<li>Added an <a href="../Object-Storage-Guide/index.html">Object Storage Guide</a> with information on how to configure OpenMPF to work
  with a custom NGINX object storage server, and how to run jobs that use an S3 object storage server. Note that the
  system properties for the custom NGINX object storage server have changed since the last release.</li>
</ul>
<h3>Upgrade to Tesseract 4.0</h3>

<ul>
<li>Both the Tesseract OCR Text Detection Component and OpenALPR License Plate Detection Components have been updated to
  use the new version of Tesseract.</li>
<li>Additionally, Leptonica has been upgraded from 1.72 to 1.75.</li>
</ul>
<h3>Docker Deployment</h3>

<ul>
<li>The Docker images now use the yum package manager to install ImageMagick6 from a public RPM repository instead of
  downloading the RPMs directly from imagemagick.org. This resolves an issue with the OpenMPF Docker build where RPMs
  on <a href="https://imagemagick.org/script/download.php">imagemagick.org</a> were no longer available.</li>
</ul>
<h3>Tesseract OCR Text Detection Component</h3>

<ul>
<li>Updated to allow the user to set a <code>TESSERACT_OEM</code> property in order to select an OCR engine mode (OEM).</li>
<li>"script/Latin" can now be specified as the <code>TESSERACT_LANGUAGE</code>. When selected, Tesseract will select all Latin
  characters, which can be from different Latin languages.</li>
</ul>
<h3>Ceph S3 Object Storage</h3>

<ul>
<li>Added support for downloading files from, and uploading files to, an S3 object storage server. The following job
  properties can be provided: <code>S3_ACCESS_KEY</code>, <code>S3_SECRET_KEY</code>, <code>S3_RESULTS_BUCKET</code>, <code>S3_UPLOAD_ONLY</code>.</li>
<li>At this time, only support for Ceph object storage has been tested. However, the Workflow Manager uses the AWS SDK for
  Java to communicate with the object store, so it is possible that other S3-compatible storage solutions may work as
  well.</li>
</ul>
<h3>ISO-8601 Timestamps</h3>

<ul>
<li>All timestamps in the JSON output object, and streaming video callbacks, are now in the ISO-8601 format (e.g. "
  2018-12-19T12:12:59.995-05:00"). This new format includes the time zone, which makes it possible to compare timestamps
  generated between systems in different time zones.</li>
<li>This change does not affect the track and detection start and stop offset times, which are still reported in
  milliseconds since the start of the video.</li>
</ul>
<h3>Reduced Redis Usage</h3>

<ul>
<li>The Workflow Manager has been refactored to reduce usage of the Redis in-memory database. In general, Redis is not
  necessary for storing job information and only resulted in introducing potential delays in accessing that data over
  the network stack.</li>
<li>Now, only track and detection data is stored in Redis for batch jobs. This reduces the amount of memory the Workflow
  Manager requires of the Java Virtual Machine. Compared to the other job information, track and detection data can
  potentially be relatively much larger. In the future, we plan to store frame data in Redis for streaming jobs as well.</li>
</ul>
<h3>Caffe Vehicle Color Estimation</h3>

<ul>
<li>The Caffe
  Component <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/CaffeDetection/plugin-files/models/models.ini">models.ini</a>
  file has been updated with a "vehicle_color" section with links for downloading
  the <a href="https://github.com/rezafuad/vehicle-color-recognition">Reza Fuad Rachmadi's Vehicle Color Recognition Using Convolutional Neural Network</a>
  model files.</li>
<li>The following pipelines have been added. These require the above model files to be placed
  in <code>$MPF_HOME/share/models/CaffeDetection</code>:<ul>
<li><code>CAFFE REZAFUAD VEHICLE COLOR DETECTION PIPELINE</code></li>
<li><code>CAFFE REZAFUAD VEHICLE COLOR DETECTION (WITH FF REGION FROM TINY YOLO VEHICLE DETECTOR) PIPELINE</code></li>
<li><code>CAFFE REZAFUAD VEHICLE COLOR DETECTION (WITH FF REGION FROM YOLO VEHICLE DETECTOR) PIPELINE</code></li>
</ul>
</li>
</ul>
<h3>Track Merging and Minimum Track Length</h3>

<ul>
<li>The following system properties now have "video" in their names:<ul>
<li><code>detection.video.track.merging.enabled</code></li>
<li><code>detection.video.track.min.gap</code></li>
<li><code>detection.video.track.min.length</code></li>
<li><code>detection.video.track.overlap.threshold</code></li>
</ul>
</li>
<li>The above properties can be overridden by the following job properties, respectively. These have not been renamed
  since the last release:<ul>
<li><code>MERGE_TRACKS</code></li>
<li><code>MIN_GAP_BETWEEN_TRACKS</code></li>
<li><code>MIN_TRACK_LENGTH</code></li>
<li><code>MIN_OVERLAP</code></li>
</ul>
</li>
<li>These system and job properties now only apply to video media. This resolves an issue where users had
  set <code>detection.track.min.length=5</code>, which resulted in dropping all image media tracks. By design, each image track can
  only contain a single detection.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>Fixed a bug where the Docker entrypoint scripts appended properties to the end
  of <code>$MPF_HOME/share/config/mpf-custom.properties</code> every time the Docker deployment was restarted, resulting in entries
  like <code>detection.segment.target.length=5000,5000,5000</code>.</li>
<li>Upgrading to Tesseract 4 fixes a bug where, when specifying <code>TESSERACT_LANGUAGE</code>, if one of the languages is Arabic,
  then Arabic must be specified last. Arabic can now be specified first, for example: <code>ara+eng</code>.</li>
<li>Fixed a bug where the minimum track length property was being applied to image tracks. Now it's only applied to video
  tracks.</li>
<li>Fixed a bug where ImageMagick6 installation failed while building Docker images.</li>
</ul>
<h1 id="openmpf-30x">OpenMPF 3.0.x</h1>
<h2>3.0.0: December 2018</h2>

<blockquote>
<p><strong>NOTE:</strong> The <a href="../Build-Environment-Setup-Guide/index.html">Build Guide</a> and <a href="../Install-Guide/index.html">Install Guide</a> are outdated. The old process for manually configuring a Build VM, using it to build an OpenMPF package, and installing that package, is deprecated in favor of Docker containers. Please refer to the openmpf-docker <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md">README</a>.</p>
<p><strong>NOTE:</strong> Do not attempt to register or unregister a component through the Nodes UI in a Docker deployment. It may appear to succeed, but the changes will not affect the child Node Manager containers, only the Workflow Manager container. Also, do not attempt to use the <code>mpf</code> command line tools in a Docker deployment.</p>
</blockquote>
<h3>Documentation</h3>

<ul>
<li>Added a <a href="https://github.com/openmpf/openmpf-docker/blob/master/README.md">README</a>
  , <a href="https://github.com/openmpf/openmpf-docker/blob/master/SWARM.md">SWARM</a> guide,
  and <a href="https://github.com/openmpf/openmpf-docker/blob/master/CONTRIBUTING.md">CONTRIBUTING</a> guide for Docker deployment.</li>
<li>Updated the <a href="../User-Guide/index.html#min_gap_between_segments-property">User Guide</a> with information on how track
  properties and track confidence are handled when merging tracks.</li>
<li>Added README files for new components. Refer to the component sections below.</li>
</ul>
<h3>Docker Support</h3>

<ul>
<li>OpenMPF can now be built and distributed as 5 Docker images: openmpf_workflow_manager, openmpf_node_manager,
  openmpf_active_mq, mysql_database, and redis.</li>
<li>These images can be deployed on a single host using <code>docker-compose up</code>.</li>
<li>They can also be deployed across multiple hosts in a Docker swarm cluster using <code>docker stack deploy</code>.</li>
<li>GPU support is enabled through the NVIDIA Docker runtime.</li>
<li>Both HTTP and HTTPS deployments are supported.</li>
</ul>
<p><span id="json-output-object"></span></p>
<h3>JSON Output Object</h3>

<ul>
<li>Added a <code>trackProperties</code> field at the track level that works in much the same way as the <code>detectionProperties</code> field
  at the detection level. Both are maps that contain zero or more key-value pairs. The component APIs have always
  supported the ability to return track-level properties, but they were never represented in the JSON output object,
  until now.</li>
<li>Similarly, added a track <code>confidence</code> field. The component APIs always supported setting it, but the value was never
  used in the JSON output object, until now.</li>
<li>Added <code>jobErrors</code> and<code>jobWarnings</code> fields. The <code>jobErrors</code> field will mention that there are items
  in <code>detectionProcessingErrors</code> fields.</li>
<li>The <code>offset</code>, <code>startOffset</code>, and <code>stopOffset</code> fields have been removed in favor of the existing <code>offsetFrame</code>
  , <code>startOffsetFrame</code>, and <code>stopOffsetFrame</code> fields, respectively. They were redundant and deprecated.</li>
<li>Added a <code>mpf.output.objects.exemplars.only</code> system property, and <code>OUTPUT_EXEMPLARS_ONLY</code> job property, that can be set
  to reduce the size of the JSON output object by only recording the track exemplars instead of all of the detections in
  each track.</li>
<li>Added a <code>mpf.output.objects.last.stage.only</code> system property, and <code>OUTPUT_LAST_STAGE_ONLY</code> job property, that can be
  set to reduce the size of the JSON output object by only recording the detections for the last non-markup stage of a
  pipeline.</li>
</ul>
<h3>Darknet Component</h3>

<ul>
<li>The Darknet component can now support processing streaming video.</li>
<li>In batch mode, video frames are prefetched, decoded, and stored in a buffer using a separate thread from the one that
  performs the detection. The size of the prefetch buffer can be configured by setting <code>FRAME_QUEUE_CAPACITY</code>.</li>
<li>The Darknet component can now perform basic tracking and generate video tracks with multiple detections. Both the
  default detection mode and preprocessor detection mode are supported.</li>
<li>The Darknet component has been updated to support the full and tiny YOLOv3 models. The YOLOv2 models are no longer
  supported.</li>
</ul>
<h3>Tesseract OCR Text Detection Component</h3>

<ul>
<li>This new component extracts text found in an image and reports it as a single-detection track.</li>
<li>PDF documents can also be processed with one track detection per page.</li>
<li>Users may set the language of each track using the <code>TESSERACT_LANGUAGE</code> property as well as adjust other image
  preprocessing properties for text extraction.</li>
<li>Refer to
  the <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/TesseractOCRTextDetection/README.md">README</a>.</li>
</ul>
<h3>OpenCV Scene Change Detection Component</h3>

<ul>
<li>This new component detects and segments a given video by scenes. Each scene change is detected using histogram
  comparison, edge comparison, brightness (fade outs), and overall hue/saturation/value differences between adjacent
  frames.</li>
<li>Users can toggle each type of of scene change detection technique as well as threshold properties for each detection
  method.</li>
<li>Refer to the <a href="https://github.com/openmpf/openmpf-components/blob/master/cpp/SceneChangeDetection/README.md">README</a>.</li>
</ul>
<h3>Tika Text Detection Component</h3>

<ul>
<li>This new component extracts text contained in documents and performs language detection. 71 languages and most
  document formats (.txt, .pptx, .docx, .doc, .pdf, etc.) are supported.</li>
<li>Refer to the <a href="https://github.com/openmpf/openmpf-components/blob/master/java/TikaTextDetection/README.md">README</a>.</li>
</ul>
<h3>Tika Image Detection Component</h3>

<ul>
<li>This new component extracts images embedded in document formats (.pdf, .ppt, .doc) and stores them on disk in a
  specified directory.</li>
<li>Refer to the <a href="https://github.com/openmpf/openmpf-components/blob/master/java/TikaImageDetection/README.md">README</a>.</li>
</ul>
<h3>Track-Level Properties and Confidence</h3>

<ul>
<li>Refer to the addition of track-level properties and confidence in the <a href="#json-output-object">JSON Output Object</a>
  section.</li>
<li>Components have been updated to return meaningful track-level properties. Caffe and Darknet include <code>CLASSIFICATION</code>,
  OALPR includes the exemplar <code>TEXT</code>, and Sphinx includes the <code>TRANSCRIPTION</code>.</li>
<li>The Workflow Manager will now populate the track-level confidence. It is the same as the exemplar confidence, which is
  the max of all of the track detections.</li>
</ul>
<h3>Custom NGINX HTTP Object Storage</h3>

<ul>
<li>Added <code>http.object.storage.*</code> system properties for configuring an optional custom NGINX object storage server on
  which to store generated detection artifacts, JSON output objects, and markup files.</li>
<li>When a file cannot be uploaded to the server, the Workflow Manager will fall back to storing it in <code>$MPF_HOME/share</code>,
  which is the default behavior when an object storage server is not specified.</li>
<li>If and when a failure occurs, the JSON output object will contain a descriptive message in the <code>jobWarnings</code> field,
  and, if appropriate, the <code>markupResult.message</code> field. If the job completes without other issues, the final status
  will be <code>COMPLETE_WITH_WARNINGS</code>.</li>
<li>The NGINX storage server runs custom server-side code which we can make available upon request. In the future, we plan
  to support more common storage server solutions, such as Amazon S3.</li>
</ul>
<p><span id="activemq"></span></p>
<h3>ActiveMQ</h3>

<ul>
<li>The <code>MPF_OUTPUT</code> queue is no longer supported and has been removed. Job producers can specify a callback URL when
  creating a job so that they are alerted when the job is complete. Users observed heap space issues with ActiveMQ after
  running thousands of jobs without consuming messages from the <code>MPF_OUTPUT</code> queue.</li>
<li>The Workflow Manager will now silently discard duplicate sub-job request messages in the ActiveMQ Dead Letter Queue (
  DLQ). This fixes a bug where the Workflow Manager would prematurely terminate jobs corresponding to the duplicate
  messages. It's assumed that ActiveMQ will only place a duplicate message in the DLQ if the original message, or
  another duplicate, can be delivered.</li>
</ul>
<h3>Node Auto-Configuration</h3>

<ul>
<li>Added the <code>node.auto.config.enabled</code>, <code>node.auto.unconfig.enabled</code>, and <code>node.auto.config.num.services.per.component</code>
  system properties for automatically managing the configuration of services when nodes join and leave the OpenMPF
  cluster.</li>
<li>Docker will assign a a hostname with a randomly-generated id to containers in a swarm deployment. The above properties
  allow the Workflow Manager to automatically discover and configure services on child Node Manager components, which is
  convenient since the hostname of those containers cannot be known in advance, and new containers with new hostnames
  are created when the swarm is restarted.</li>
</ul>
<h3>Job Status Web UI</h3>

<ul>
<li>Added the <code>web.broadcast.job.status.enabled</code> and <code>web.job.polling.interval</code> system properties that can be used to
  configure if the Workflow Manager automatically broadcasts updates to the Job Status web UI. By default, the
  broadcasts are enabled.</li>
<li>In a production environment that processes hundreds of jobs or more at the same time, this behavior can result in
  overloading the web UI, causing it to slow down and freeze up. To prevent this, set <code>web.broadcast.job.status.enabled</code>
  to <code>false</code>. If <code>web.job.polling.interval</code> is set to a non-zero value, the web UI will poll for updates at that
  interval (specified in milliseconds).</li>
<li>To disable broadcasts and polling, set <code>web.broadcast.job.status.enabled</code> to <code>false</code> and <code>web.job.polling.interval</code> to
  a zero or negative value. Users will then need to manually refresh the Job Status web page using their web browser.</li>
</ul>
<h3>Other Improvements</h3>

<ul>
<li>Now using variable-length text fields in the mySQL database for string data that may exceed 255 characters.</li>
<li>Updated the MPFImageReader tool to use OpenCV video capture behind the scenes to support reading data from HTTP URLs.</li>
<li>Python components can now include pre-built wheel files in the plugin package.</li>
<li>We now use a <a href="https://github.com/openmpf/openmpf-docker/blob/master/Jenkinsfile">Jenkinsfile</a> Groovy script for our
  Jenkins build process. This allows us to use revision control for our continuous integration process and share that
  process with the open source community.</li>
<li>Added <code>remote.media.download.retries</code> and <code>remote.media.download.sleep</code> system properties that can be used to
  configure how the Workflow Manager will attempt to retry downloading remote media if it encounters a problem.</li>
<li>Artifact extraction now uses MPFVideoCapture, which employs various fallback strategies for extracting frames in cases
  where a video is not well-formed or corrupted. For components that use MPFVideoCapture, this enables better
  consistency between the frames they process and the artifacts that are later extracted.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>Jobs now properly end in <code>ERROR</code> if an invalid media URL is provided or there is a problem accessing remote media.</li>
<li>Jobs now end in <code>COMPLETE_WITH_ERRORS</code> when a detection splitter error occurs due to missing system properties.</li>
<li>Components can now include their own version of the Google Protobuf library. It will not conflict with the version
  used by the rest of OpenMPF.</li>
<li>The Java component executor now sets the proper job id in the job name instead of using the ActiveMQ message request
  id.</li>
<li>The Java component executor now sets the run directory using <code>setRunDirectory()</code>.</li>
<li>Actions can now be properly added using an "extras" component. An extras component only includes a <code>descriptor.json</code>
  file and declares Actions, Tasks, and Pipelines using other component algorithms.</li>
<li>Refer to the items listed in the <a href="#activemq">ActiveMQ</a> section.</li>
<li>Refer to the addition of track-level properties and confidence in the <a href="#json-output-object">JSON Output Object</a>
  section.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>[<a href="https://github.com/openmpf/openmpf/issues/745">#745</a>] In environments where thousands of jobs are processed, users
  have observed that, on occasion, pending sub-job messages in ActiveMQ queues are not processed until a new job is
  created. The reason is currently unknown.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/544">#544</a>] Image artifacts retain some permissions from source files
  available on the local host. This can result in some of the image artifacts having executable permissions.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/604">#604</a>] The Sphinx component cannot be unregistered
  because <code>$MPF_HOME/plugins/SphinxSpeechDetection/lib</code> is owned by root on a deployment machine.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/623">#623</a>] The Nodes UI does not work correctly
  when <code>[POST] /rest/nodes/config</code> is used at the same time. This is because the UI's state is not automatically updated
  to reflect changes made through the REST endpoint.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/783">#783</a>] The Tesseract OCR Text Detection Component has
  a <a href="https://github.com/tesseract-ocr/tesseract/issues/235">known issue</a> because it uses Tesseract 3. If a combination
  of languages is specified using <code>TESSERACT_LANGUAGE</code>, and one of the languages is Arabic, then Arabic must be
  specified last. For example, for English and Arabic, <code>eng+ara</code> will work, but <code>ara+eng</code> will not.</li>
<li>[<a href="https://github.com/openmpf/openmpf/issues/784">#784</a>] Sometimes services do not start on OpenMPF nodes, and those
  services cannot be started through the Nodes web UI. This is not a Docker-specific problem, but it has been observed
  in a Docker swarm deployment when auto-configuration is enabled. The workaround is to restart the Docker swarm
  deployment, or remove the entire node in the Nodes UI and add it again.</li>
</ul>
<h1 id="openmpf-21x">OpenMPF 2.1.x</h1>
<h2>2.1.0: June 2018</h2>

<blockquote>
<p><strong>NOTE:</strong> If building this release on a machine used to build a previous version of OpenMPF, then please run <code>sudo pip install --upgrade pip</code> to update to at least pip 10.0.1. If not, the OpenMPF build script will fail to properly download .whl files for Python modules.</p>
</blockquote>
<h3>Documentation</h3>

<ul>
<li>Added the <a href="../Python-Batch-Component-API/index.html">Python Batch Component API</a>.</li>
<li>Added the <a href="../Node-Guide/index.html">Node Guide</a>.</li>
<li>Added the <a href="../GPU-Support-Guide">GPU Support Guide</a>.</li>
<li>Updated the <a href="../Install-Guide/index.html">Install Guide</a> with an "(Optional) Install the NVIDIA CUDA Toolkit" section.</li>
<li>Renamed Admin Manual to Admin Guide for consistency.</li>
</ul>
<h3>Python Batch Component API</h3>

<ul>
<li>Developers can now write batch components in Python using the mpf_component_api module.</li>
<li>Dependencies can be specified in a setup.py file. OpenMPF will automatically download the .whl files using pip at
  build time.</li>
<li>When deployed, a virtualenv is created for the Python component so that it runs in a sandbox isolated from the rest of
  the system.</li>
<li>OpenMPF ImageReader and VideoCapture tools are provided in the mpf_component_util module.</li>
<li>Example Python components are provided for reference.</li>
</ul>
<h3>Spare Nodes</h3>

<ul>
<li>Spare nodes can join and leave an OpenMPF cluster while the Workflow Manager is running. You can create a spare node
  by cloning an existing OpenMPF child node. Refer to the <a href="../Node-Guide/index.html">Node Guide</a>.</li>
<li>Note that changes made using the Component Registration web page only affect core nodes, not spare nodes. Core nodes
  are those configured during the OpenMPF installation process.</li>
<li>Added <code>mpf list-nodes</code> command to list the core nodes and available spare nodes.</li>
<li>OpenMPF now uses the JGroups FILE_PING protocol for peer discovery instead of TCPPING. This means that the list of
  OpenMPF nodes no longer needs to be fully specified when the Workflow Manager starts. Instead, the Workflow Manager,
  and Node Manager process on each node, use the files in <code>$MPF_HOME/share/nodes</code> to determine which nodes are currently
  available.</li>
<li>Updated JGroups from 3.6.4. to 4.0.11.</li>
<li>The environment variables specified in <code>/etc/profile.d/mpf.sh</code> have been simplified. Of note, <code>ALL_MPF_NODES</code> has been
  replaced by <code>CORE_MPF_NODES</code>.</li>
</ul>
<h3>Default Detection System Properties</h3>

<ul>
<li>The detection properties that specify the default values when creating new jobs can now be updated at runtime without
  restarting the Workflow Manager. Changing these properties will only have an effect on new jobs, not jobs that are
  currently running.</li>
<li>These default detection system properties are separated from the general system properties in the Properties web page.
  The latter still require the Workflow Manager to be restarted for changes to take effect.</li>
<li>The Apache Commons Configuration library is now used to read and write properties files. When defining a property
  value using an environment variable in the Properties web page, or <code>$MPF_HOME/config/mpf-custom.properties</code>, be sure
  to prepend the variable name with <code>env:</code>. For example:</li>
</ul>
<pre><code>detection.models.dir.path=${env:MPF_HOME}/models/
</code></pre>
<blockquote>
<p>Alternatively, you can define system properties using other system properties:</p>
</blockquote>
<pre><code>detection.models.dir.path=${mpf.share.path}/models/
</code></pre>
<h3>Adaptive Frame Interval</h3>

<ul>
<li>The <code>FRAME_RATE_CAP</code> property can be used to set a threshold on the maximum number of frames to process within one
  second of the native video time. This property takes precedence over the user-provided / pipeline-provided value
  for <code>FRAME_INTERVAL</code>. When the <code>FRAME_RATE_CAP</code> property is specified, an internal frame interval value is calculated
  as follows:</li>
</ul>
<pre><code>calcFrameInterval = max(1, floor(mediaNativeFPS / frameRateCapProp));
</code></pre>
<ul>
<li><code>FRAME_RATE_CAP</code> may be disabled by setting it &lt;= 0. <code>FRAME_INTERVAL</code> can be disabled in the same way.</li>
<li>If <code>FRAME_RATE_CAP</code> is disabled, then <code>FRAME_INTERVAL</code> will be used instead.</li>
<li>If both <code>FRAME_RATE_CAP</code> and <code>FRAME_INTERVAL</code> are disabled, then a value of 1 will be used for <code>FRAME_INTERVAL</code>.</li>
</ul>
<h3>Darknet Component</h3>

<ul>
<li>This release includes a component that uses the <a href="https://pjreddie.com/darknet/">Darknet neural network framework</a> to
  perform detection and classification of objects using trained models.</li>
<li>Pipelines for the Tiny YOLO and YOLOv2 models are provided. Due to its large size, the YOLOv2 weights file must be
  downloaded separately and placed in <code>$MPF_HOME/share/models/DarknetDetection</code> in order to use the YOLOv2 pipelines.
  Refer to <code>DarknetDetection/plugin-files/models/models.ini</code> for more information.</li>
<li>This component supports a preprocessor mode and default mode of operation. If preprocessor mode is enabled, and
  multiple Darknet detections in a frame share the same classification, then those are merged into a single detection
  where the region corresponds to the superset region that encapsulates all of the original detections, and the
  confidence value is the probability that at least one of the original detections is a true positive. If disabled,
  multiple Darknet detections in a frame are not merged together.</li>
<li>Detections are not tracked across frames. One track is generated per detection.</li>
<li>This component supports an optional <code>CLASS_WHITELIST_FILE</code> property. When provided, only detections with class names
  listed in the file will be generated.</li>
<li>This component can be compiled with GPU support if the NVIDIA CUDA Toolkit is installed on the build machine. Refer to
  the <a href="../GPU-Support-Guide">GPU Support Guide</a>. If the toolkit is not found, then the component will compile with CPU
  support only.</li>
<li>To run on a GPU, set the <code>CUDA_DEVICE_ID</code> job property, or set the detection.cuda.device.id system property, &gt;= 0.</li>
<li>When <code>CUDA_DEVICE_ID</code> &gt;= 0, you can set the <code>FALLBACK_TO_CPU_WHEN_GPU_PROBLEM</code> job property, or the
  detection.use.cpu.when.gpu.problem system property, to <code>TRUE</code> if you want to run the component logic on the CPU
  instead of the GPU when a GPU problem is detected.</li>
</ul>
<h3>Models Directory</h3>

<ul>
<li>The<code>$MPF_HOME/share/models</code> directory is now used by the Darknet and Caffe components to store model files and
  associated files, such as classification names files, weights files, etc. This allows users to more easily add model
  files post-deployment. Instead of copying the model files to <code>$MPF_HOME/plugins/&lt;component-name&gt;/models</code> directory on
  each node in the OpenMPF cluster, they only need to copy them to the shared directory once.</li>
<li>To add new models to the Darknet and Caffe component, add an entry to the
  respective <code>&lt;component-name&gt;/plugin-files/models/models.ini</code> file.</li>
</ul>
<h3>Packaging and Deployment</h3>

<ul>
<li>Python components are packaged with their respective dependencies as .whl files. This can be automated by providing a
  setup.py file. An example OpenCV Python component is provided that demonstrates how the component is packaged and
  deployed with the opencv-python module. When deployed, a virtualenv is created for the component with the .whl files
  installed in it.</li>
<li>When deploying OpenMPF, <code>LD_LIBRARY_PATH</code> is no longer set system-wide. Refer to Known Issues.</li>
</ul>
<h3>Web User Interface</h3>

<ul>
<li>Updated the Nodes page to distinguish between core nodes and spare nodes, and to show when a node is online or
  offline.</li>
<li>Updated the Component Registration page to list the core nodes as a reminder that changes will not affect spare nodes.</li>
<li>Updated the Properties page to separate the default detection properties from the general system properties.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>Custom Action, task, and pipeline names can now contain "(" and ")" characters again.</li>
<li>Detection location elements for audio tracks and generic tracks in a JSON output object will now have a y value of <code>0</code>
  instead of <code>1</code>.</li>
<li>Streaming health report and summary report timestamps have been corrected to represent hours in the 0-23 range instead
  of 1-24.</li>
<li>Single-frame .gif files are now segmented properly and no longer result in a NullPointerException.</li>
<li><code>LD_LIBRARY_PATH</code> is now set at the process level for Tomcat, the Node Manager, and component services, instead of at
  the system level in <code>/etc/profile.d/mpf.sh</code>. Also, deployments no longer create <code>/etc/ld.so.conf.d/mpf.conf</code>. This
  better isolates OpenMPF from the rest of the system and prevents issues, such as being unable to use SSH, when system
  libraries are not compatible with OpenMPF libraries. The latter situation may occur when running <code>yum update</code> on the
  system, which can make OpenMPF unusable until a new deployment package with compatible libraries is installed.</li>
<li>The Workflow Manager will no longer generate an "Error retrieving the SingleJobInfo model" line in the log if someone
  is viewing the Job Status page when a job submitted through the REST API is in progress.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>When multiple component services of the same type on the same node log to the same file at the same time, sometimes
  log lines will not be captured in the log file. The logging frameworks (log4j and log4cxx) do not support that usage.
  This problem happens more frequently on systems running many component services at the same time.</li>
<li>The following exception was observed:</li>
</ul>
<pre><code>com.google.protobuf.InvalidProtocolBufferException: Message missing required fields: data_uri

</code></pre>
<blockquote>
<p>Further debugging is necessary to determine the reason why that message was missing that field. The situation is not easily reproducible. It may occur when ActiveMQ and / or the system is under heavy load and sends duplicate messages in attempt to ensure message delivery. Some of those messages seem to end up in the dead letter queue (DLQ). For now, we've improved the way we handle messages in the DLQ. If OpenMPF can process a message successfully, the job is marked as <code>COMPLETED_WITH_ERRORS</code>, and the message is moved from <code>ActiveMQ.DLQ</code> to <code>MPF.DLQ_PROCESSED_MESSAGES</code>. If OpenMPF cannot process a message successfully, it is moved from <code>ActiveMQ.DLQ to MPF.DLQ_INVALID_MESSAGES</code>.</p>
</blockquote>
<ul>
<li>The <code>mpf stop</code> command will stop the Workflow Manager, which will in turn send commands to all of the available nodes
  to stop all running component services. If a service is processing a sub-job when the quit command is received, that
  service process will not terminate until that sub-job is completely processed. Thus, the service may put a sub-job
  response on the ActiveMQ response queue after the Workflow Manager has terminated. That will not cause a problem
  because the queues are flushed the next time the Workflow Manager starts; however, there will be a problem if the
  service finishes processing the sub-job after the Workflow Manager is restarted. At that time, the Workflow Manager
  will have no knowledge of the old job and will in turn generate warnings in the log about how the job id is "not known
  to the system" and/or "not found as a batch or a streaming job". These can be safely ignored. Often, if these messages
  appear in the log, then C++ services were running after stopping the Workflow Manager. To address this, you may wish
  to run <code>sudo killall amq_detection_component</code> after running <code>mpf stop</code>.</li>
</ul>
<h1 id="openmpf-20x">OpenMPF 2.0.x</h1>
<h2>2.0.0: February 2018</h2>

<blockquote>
<p><strong>NOTE:</strong> Components built for previous releases of OpenMPF are not compatible with OpenMPF 2.0.0 due to Batch Component API changes to support generic detections, and changes made to the format of the <code>descriptor.json</code> file to support stream processing.</p>
<p><strong>NOTE:</strong> This release contains basic support for processing video streams. Currently, the only way to make use of that functionality is through the REST API. Streaming jobs and services cannot be created or monitored through the web UI. Only the SuBSENSE component has been updated to support streaming. Only single-stage pipelines are supported at this time.</p>
</blockquote>
<h3>Documentation</h3>

<ul>
<li>Updated documents to distinguish the batch component APIs from the streaming component API.</li>
<li>Added the <a href="../CPP-Streaming-Component-API/index.html">C++ Streaming Component API</a>.</li>
<li>Updated the <a href="../CPP-Batch-Component-API/index.html">C++ Batch Component API</a> to describe support for generic detections.</li>
<li>Updated the <a href="../REST-API/index.html">REST API</a> with endpoints for streaming jobs.</li>
</ul>
<h3>Support for Generic Detections</h3>

<ul>
<li>C++ and Java components can now declare support for the <code>UNKNOWN</code> data type. The respective batch APIs have been
  updated with a function that will enable a component to process an <code>MPFGenericJob</code>, which represents a piece of media
  that is not a video, image, or audio file.</li>
<li>Note that these API changes make OpenMPF R2.0.0 incompatible with components built for previous releases of OpenMPF.
  Specifically, the new component executor will not be able to load the component logic library.</li>
</ul>
<h3>C++ Batch Component API</h3>

<ul>
<li>Added the following function to support generic detections:<ul>
<li><code>MPFDetectionError GetDetections(const MPFGenericJob &amp;job, vector&lt;MPFGenericTrack&gt; &amp;tracks)</code></li>
</ul>
</li>
</ul>
<h3>Java Batch Component API</h3>

<ul>
<li>Added the following method to support generic detections:<ul>
<li><code>List&lt;MPFGenericTrack&gt; getDetections(MPFGenericJob job)</code></li>
</ul>
</li>
</ul>
<h3>Streaming REST API</h3>

<ul>
<li>Added the following REST endpoints for streaming jobs:<ul>
<li><code>[GET] /rest/streaming/jobs</code>: Returns a list of streaming job ids.</li>
<li><code>[POST] /rest/streaming/jobs</code>: Creates and submits a streaming job. Users can register for health report and
  summary report callbacks.</li>
<li><code>[GET] /rest/streaming/jobs/{id}</code>: Gets information about a streaming job.</li>
<li><code>[POST] /rest/streaming/jobs/{id}/cancel</code>: Cancels a streaming job.</li>
</ul>
</li>
</ul>
<h3>Workflow Manager</h3>

<ul>
<li>Updated to support generic detections.</li>
<li>Updated Redis to store information about streaming jobs.</li>
<li>Added controllers for streaming job REST endpoints.</li>
<li>Added ability to generate health reports and segment summary reports for streaming jobs.</li>
<li>Improved code flow between the Workflow Manager and master Node Manager to support streaming jobs.</li>
<li>Added ActiveMQ queues to enable the C++ Streaming Component Executor to send reports and job status to the Workflow
  Manager.</li>
</ul>
<h3>Node Manager</h3>

<ul>
<li>Updated the master Node Manager and child Node Managers to spawn component services on demand to handle streaming
  jobs, cancel those jobs, and to monitor the status of those processes.</li>
<li>Using .ini files to represent streaming job properties and enable better communication between a child Node Manager
  and C++ Streaming Component Executor.</li>
</ul>
<h3>C++ Streaming Component API</h3>

<ul>
<li>Developed the C++ Streaming Component API with the following functions:<ul>
<li><code>MPFStreamingDetectionComponent(const MPFStreamingVideoJob &amp;job)</code>: Constructor that takes a streaming video job.</li>
<li><code>string GetDetectionType()</code>: Returns the type of detection (i.e. "FACE").</li>
<li><code>void BeginSegment(const VideoSegmentInfo &amp;segment_info)</code>: Indicates the beginning of a new video segment.</li>
<li><code>bool ProcessFrame(const cv::Mat &amp;frame, int frame_number)</code>: Processes a single frame for the current video
  segment.</li>
<li><code>vector&lt;MPFVideoTrack&gt; EndSegment()</code>: Indicates the end of the current video segment.</li>
</ul>
</li>
<li>Updated the C++ Hello World component to support streaming jobs.</li>
</ul>
<h3>C++ Streaming Component Executor</h3>

<ul>
<li>Developed the C++ Streaming Component Executor to load a streaming component logic library, read frames from a video
  stream, and exercise the component logic through the C++ Streaming Component API.</li>
<li>When the C++ Streaming Component Executor cannot read a frame from the stream, it will sleep for at least 1
  millisecond, doubling the amount of sleep time per attempt until it reaches the  <code>stallTimeout</code> value specified when
  the job was created. While stalled, the job status will be <code>STALLED</code>. After the timeout is exceeded, the job will
  be <code>TERMINATED</code>.</li>
<li>The C++ Streaming Component Executor supports <code>FRAME_INTERVAL</code>, as well as rotation, horizontal flipping, and
  cropping (region of interest) properties. Does not support <code>USE_KEY_FRAMES</code>.</li>
</ul>
<h3>Interoperability Package</h3>

<ul>
<li>Added the following Java classes to the interoperability package to simplify third party integration:<ul>
<li><code>JsonHealthReportCollection</code>: Represents the JSON content of a health report callback. Contains one or
  more <code>JsonHealthReport</code> objects.</li>
<li><code>JsonSegmentSummaryReport</code>: Represents the JSON content of a summary report callback. Content is similar to the
  JSON output object used for batch processing.</li>
</ul>
</li>
</ul>
<h3>SuBSENSE Component</h3>

<ul>
<li>The SuBSENSE component now supports both batch processing and stream processing.</li>
<li>Each video segment will be processed independently of the rest. In other words, tracks will be generated on a
  segment-by-segment basis and tracks will not carry over between segments.</li>
<li>Note that the last frame in the previous segment will be used to determine if there is motion in the first frame of
  the next segment.</li>
</ul>
<h3>Packaging and Deployment</h3>

<ul>
<li>Updated <code>descriptor.json</code> fields to allow components to support batch and/or streaming jobs. Components that use the
  old <code>descriptor.json</code> file format cannot be registered through the web UI.</li>
<li>Batch component logic and streaming component logic are compiled into separate libraries.</li>
<li>The mySQL <code>streaming_job_request</code> table has been updated with the following fields, which are used to populate the
  JSON health reports:<ul>
<li><code>status_detail</code>: (Optional) A user-friendly description of the current job status.</li>
<li><code>activity_frame_id</code>: The frame id associated with the last job activity. Activity is defined as the start of a new
  track for the current segment.</li>
<li><code>activity_timestamp</code>: The timestamp associated with the last job activity.</li>
</ul>
</li>
</ul>
<h3>Web User Interface</h3>

<ul>
<li>Added column names to the table that appears when the user clicks in the Media button associated with a job on the Job
  Status page. Now descriptive comments are provided when table cells are empty.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>Upgraded Tika to 1.17 to resolve an issue with improper indentation in a Python file (rotation.py) that resulted in
  generating at least one error message per image processed. When processing a large number of images, this would
  generate may error messages, causing the Automatic Bug Reporting Tool daemon (abrtd) process to run at 100% CPU. Once
  in that state, that process would stay there, essentially wasting on CPU core. This resulted in some of the Jenkins
  virtual machines we used for testing to become unresponsive.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>
<p>OpenCV 3.3.0 <code>cv::imread()</code> does not properly decode some TIFF images that have EXIF orientation metadata. It can
  handle images that are flipped horizontally, but not vertically. It also has issues with rotated images. Since most
  components rely on that function to read image data, those components may silently fail to generate detections for
  those kinds of images.</p>
</li>
<li>
<p>Using single quotes, apsotrophes, or double quotes in the name of an algorithm, action, task, or pipeline configured
  on an existing OpenMPF system will result in a failure to perform an OpenMPF upgrade on that system. Specifically, the
  step where pre-existing custom actions, tasks, and pipelines are carried over to the upgraded version of OpenMPF will
  fail. Please do not use those special characters while naming those elements. If this has been done already, then
  those elements should be manually renamed in the XML files prior to an upgrade attempt.</p>
</li>
<li>
<p>OpenMPF uses OpenCV, which uses FFmpeg, to connect to video streams. If a proxy and/or firewall prevents the network
  connection from succeeding, then OpenCV, or the underlying FFmpeg library, will segfault. This causes the C++
  Streaming Component Executor process to fail. In turn, the job status will be set to <code>ERROR</code> with a status detail
  message of "Unexpected error. See logs for details". In this case, the logs will not contain any useful information.
  You can identify a segfault by the following line in the node-manager log:</p>
</li>
</ul>
<pre><code>2018-02-15 16:01:21,814 INFO [pool-3-thread-4] o.m.m.nms.streaming.StreamingProcess - Process: Component exited with exit code 139
</code></pre>
<blockquote>
<p>To determine if FFmpeg can connect to the stream or not, run <code>ffmpeg -i &lt;stream-uri&gt;</code> in a terminal window. Here's an example when it's successful:</p>
</blockquote>
<pre><code class="language-bash">[mpf@localhost bin]$ ffmpeg -i rtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov
ffmpeg version n3.3.3-1-ge51e07c Copyright (c) 2000-2017 the FFmpeg developers
  built with gcc 4.8.5 (GCC) 20150623 (Red Hat 4.8.5-4)
  configuration: --prefix=/apps/install --extra-cflags=-I/apps/install/include --extra-ldflags=-L/apps/install/lib --bindir=/apps/install/bin --enable-gpl --enable-nonfree --enable-libtheora --enable-libfreetype --enable-libmp3lame --enable-libvorbis --enable-libx264 --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-version3 --enable-shared --disable-libsoxr --enable-avresample
  libavutil      55. 58.100 / 55. 58.100
  libavcodec     57. 89.100 / 57. 89.100
  libavformat    57. 71.100 / 57. 71.100
  libavdevice    57.  6.100 / 57.  6.100
  libavfilter     6. 82.100 /  6. 82.100
  libavresample   3.  5.  0 /  3.  5.  0
  libswscale      4.  6.100 /  4.  6.100
  libswresample   2.  7.100 /  2.  7.100
  libpostproc    54.  5.100 / 54.  5.100
[rtsp @ 0x1924240] UDP timeout, retrying with TCP
Input #0, rtsp, from 'rtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov':
  Metadata:
    title           : BigBuckBunny_115k.mov
  Duration: 00:09:56.48, start: 0.000000, bitrate: N/A
    Stream #0:0: Audio: aac (LC), 12000 Hz, stereo, fltp
    Stream #0:1: Video: h264 (Constrained Baseline), yuv420p(progressive), 240x160, 24 fps, 24 tbr, 90k tbn, 48 tbc
At least one output file must be specified
</code></pre>
<blockquote>
<p>Here's an example when it's not successful, so there may be network issues:</p>
</blockquote>
<pre><code class="language-bash">[mpf@localhost bin]$ ffmpeg -i rtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov
ffmpeg version n3.3.3-1-ge51e07c Copyright (c) 2000-2017 the FFmpeg developers
  built with gcc 4.8.5 (GCC) 20150623 (Red Hat 4.8.5-4)
  configuration: --prefix=/apps/install --extra-cflags=-I/apps/install/include --extra-ldflags=-L/apps/install/lib --bindir=/apps/install/bin --enable-gpl --enable-nonfree --enable-libtheora --enable-libfreetype --enable-libmp3lame --enable-libvorbis --enable-libx264 --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-version3 --enable-shared --disable-libsoxr --enable-avresample
  libavutil      55. 58.100 / 55. 58.100
  libavcodec     57. 89.100 / 57. 89.100
  libavformat    57. 71.100 / 57. 71.100
  libavdevice    57.  6.100 / 57.  6.100
  libavfilter     6. 82.100 /  6. 82.100
  libavresample   3.  5.  0 /  3.  5.  0
  libswscale      4.  6.100 /  4.  6.100
  libswresample   2.  7.100 /  2.  7.100
  libpostproc    54.  5.100 / 54.  5.100
[tcp @ 0x171c300] Connection to tcp://184.72.239.149:554?timeout=0 failed: Invalid argument
rtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov: Invalid argument
</code></pre>
<ul>
<li>Tika 1.17 does not come pre-packaged with support for some embedded image formats in PDF files, possibly to avoid
  patent issues. OpenMPF does not handle embedded images in PDFs, so that's not a problem. Tika will print out the
  following warnings, which can be safely ignored:</li>
</ul>
<pre><code>Jan 22, 2018 11:02:15 AM org.apache.tika.config.InitializableProblemHandler$3 handleInitializableProblem
WARNING: JBIG2ImageReader not loaded. jbig2 files will be ignored
See https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io
for optional dependencies.
TIFFImageWriter not loaded. tiff files will not be processed
See https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io
for optional dependencies.
J2KImageReader not loaded. JPEG2000 files will not be processed.
See https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io
for optional dependencies.

</code></pre>
<h1 id="openmpf-10x">OpenMPF 1.0.x</h1>
<h2>1.0.0: October 2017</h2>

<h3>Documentation</h3>

<ul>
<li>Updated the <a href="../Build-Environment-Setup-Guide/index.html">Build Guide</a> with instructions for installing the latest JDK,
  latest JRE, FFmpeg 3.3.3, new codecs, and OpenCV 3.3.</li>
<li>Added an <a href="../Acknowledgements/index.html">Acknowledgements</a> section that provides information on third party dependencies
  leveraged by the OpenMPF.</li>
<li>Added a <a href="../Feed-Forward-Guide/index.html">Feed Forward Guide</a> that explains feed forward processing and how to use it.</li>
<li>Added missing requirements checklist content to
  the <a href="../Install-Guide/index.html#pre-installation-checklist">Install Guide</a>.</li>
<li>Updated the README at the top level of each of the primary repositories to help with user navigation and provide
  general information.</li>
</ul>
<h3>Upgrade to FFmpeg 3.3.3 and OpenCV 3.3</h3>

<ul>
<li>Updated core framework from FFmpeg 2.6.3 to FFmpeg 3.3.3.</li>
<li>Added the following FFmpeg codecs: x256, VP9, AAC, Opus, Speex.</li>
<li>Updated core framework and components from OpenCV 3.2 to OpenCV 3.3. No longer building with opencv_contrib.</li>
</ul>
<h3>Feed Forward Behavior</h3>

<ul>
<li>Updated the workflow manager (WFM) and all video components to optionally perform feed forward processing for batch
  jobs. This allows tracks to be passed forward from one pipeline stage to the next. Components in the next stage will
  only process the frames associated with the detections in those tracks. This differs from the default segmenting
  behavior, which does not preserve detection regions or track information between stages.</li>
<li>To enable this behavior, the optional <code>FEED_FORWARD_TYPE</code> property must be set to <code>FRAME</code>, <code>SUPERSET_REGION</code>,
  or <code>REGION</code>. If set to <code>FRAME</code> then the components in the next stage will process the whole frame region associated
  with each detection in the track passed forward. If set to <code>SUPERSET_REGION</code> then the components in the next stage
  will determine the bounding box that encapsulates all of the detection regions in the track, and only process the
  pixel data within that superset region. If set to <code>REGION</code> then the components in the next stage will process the
  region associated with each detection in the track passed forward, which may vary in size and position from frame to
  frame.</li>
<li>The optional <code>FEED_FORWARD_TOP_CONFIDENCE_COUNT</code> property can be set to a number to limit the number of detections
  passed forward in a track. For example, if set to "5", then only the top 5 detections in the track will be passed
  forward and processed by the next stage. The top detections are defined as those with the highest confidence values,
  or if the confidence values are the same, those with the lowest frame index.</li>
<li>Note that setting the feed forward properties has no effect on the first pipeline stage because there is no prior
  stage that can pass tracks to it.</li>
</ul>
<h3>Caffe Component</h3>

<ul>
<li>Updated the Caffe component to process images in the BGR color space instead of the RGB color space. This addresses a
  bug found in OpenCV. Refer to the Bug Fixes section below.</li>
<li>Added support for processing videos.</li>
<li>Added support for an optional <code>ACTIVATION_LAYER_LIST</code> property. For each network layer specified in the list,
  the <code>detectionProperties</code> map in the JSON output object will contain one entry. The value is an encoded string of the
  JSON representation of an OpenCV matrix of the activation values for that layer. The activation values are obtained
  after the Caffe network has processed the frame data.</li>
<li>Added support for an optional <code>SPECTRAL_HASH_FILE_LIST</code> property. For each JSON file specified in the list,
  the <code>detectionProperties</code> map in the JSON output object will contain one entry. The value is a string of 0's and 1's
  representing the spectral hash calculated using the information in the spectral hash JSON file. The spectral hash is
  calculated using activation values after the Caffe network has processed the frame data.</li>
<li>Added a pipeline to showcase the above two features for the GoogLeNet Caffe model.</li>
<li>Removed the <code>TRANSPOSE</code> property from the Caffe component since it was not necessary.</li>
<li>Added red, green, and blue mean subtraction values to the GoogLeNet pipeline.</li>
</ul>
<h3>Use Key Frames</h3>

<ul>
<li>Added support for an optional <code>USE_KEY_FRAMES</code> property to each video component. When true the component will only
  look at key frames (I-frames) from the input video. Can be used in conjunction with <code>FRAME_INTERVAL</code>. For example,
  when <code>USE_KEY_FRAMES</code> is true, and <code>FRAME_INTERVAL</code> is set to "2", then every other key frame will be processed.</li>
</ul>
<h3>MPFVideoCapture and MPFImageReader Tools</h3>

<ul>
<li>Updated the MPFVideoCapture and MPFImageReader tools to handle feed forward properties.</li>
<li>Updated the MPFVideoCapture tool to handle <code>FRAME_INTERVAL</code> and <code>USE_KEY_FRAMES</code> properties.</li>
<li>Updated all existing components to leverage these tools as much as possible.</li>
<li>We encourage component developers to use these tools to automatically take care of common frame grabbing and frame
  manipulation behaviors, and not to reinvent the wheel.</li>
</ul>
<h3>Dead Letter Queue</h3>

<ul>
<li>If for some reason a sub-job request that should have gone to a component ends up on the ActiveMQ Dead Letter Queue (
  DLQ), then the WFM will now process that failed request so that the job can complete. The ActiveMQ management page
  will now show that <code>ActiveMQ.DLQ</code> has 1 consumer. It will also show unconsumed messages
  in <code>MPF.PROCESSED_DLQ_MESSAGES</code>. Those are left for auditing purposes. The "Message Detail" for these shows the string
  representation of the original job request protobuf message.</li>
</ul>
<h3>Upgrade Path</h3>

<ul>
<li>Removed the Release 0.8 to Release 0.9 upgrade path in the deployment scripts.</li>
<li>Added support for a Release 0.9 to Release 1.0.0 upgrade path, and a Release 0.10.0 to Release 1.0.0 upgrade path.</li>
</ul>
<h3>Markup</h3>

<ul>
<li>Bounding boxes are now drawn along the interpolated path between detection regions whenever there are one or more
  frames in a track which do not have detections associated with them.</li>
<li>For each track, the color of the bounding box is now a randomly selected hue in the HSV color space. The colors are
  evenly distributed using the golden ratio.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>Fixed a <a href="https://github.com/opencv/opencv/issues/9625">bug in OpenCV</a> where the Caffe example code was processing
  images in the RGB color space instead of the BGR color space. Updated the OpenMPF Caffe component accordingly.</li>
<li>Fixed a bug in the OpenCV person detection component that caused bounding boxes to be too large for detections near
  the edge of a frame.</li>
<li>Resubmitting jobs now properly carries over configured job properties.</li>
<li>Fixed a bug in the build order of the OpenMPF project so that test modules that the WFM depends on are built before
  the WFM itself.</li>
<li>The Markup component draws bounding boxes between detections when a <code>FRAME_INTERVAL</code> is specified. This is so that the
  bounding box in the marked-up video appears in every frame. Fixed a bug where the bounding boxes drawn on
  non-detection frames appeared to stand still rather than move along the interpolated path between detection regions.</li>
<li>Fixed a bug on the OALPR license plate detection component where it was not properly handling the <code>SEARCH_REGION_*</code>
  properties.</li>
<li>Support for the <code>MIN_GAP_BETWEEN_SEGMENTS</code> property was not implemented properly. When the gap between two segments is
  less than this property value then the segments should be merged; otherwise, the segments should remain separate. In
  some cases, the exact opposite was happening. This bug has been fixed.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>Because of the number of additional ActiveMQ messages involved, enabling feed forward for low resolution video may
  take longer than the non-feed-forward behavior.</li>
</ul>
<h1 id="openmpf-0xx">OpenMPF 0.x.x</h1>
<h2>0.10.0: July 2017</h2>

<blockquote>
<p><strong>WARNING:</strong> There is no longer a <code>DEFAULT CAFFE ACTION</code>, <code>DEFAULT CAFFE TASK</code>, or <code>DEFAULT CAFFE PIPELINE</code>. There is now a <code>CAFFE GOOGLENET DETECTION PIPELINE</code> and <code>CAFFE YAHOO NSFW DETECTION PIPELINE</code>, which each have a respective action and task.</p>
<p><strong>NOTE:</strong> MPFImageReader has been re-enabled in this version of OpenMPF since we upgraded to OpenCV 3.2, which addressed the known issues with <code>imread()</code>, auto-orientation, and jpeg files in OpenCV 3.1.</p>
</blockquote>
<h3>Documentation</h3>

<ul>
<li>Added a <a href="../Contributor-Guide/index.html">Contributor Guide</a> that provides guidelines for contributing to the OpenMPF
  codebase.</li>
<li>Updated the <a href="../Java-Batch-Component-API/index.html">Java Batch Component API</a> with links to the example Java components.</li>
<li>Updated the <a href="../Build-Environment-Setup-Guide/index.html">Build Guide</a> with instructions for OpenCV 3.2.</li>
</ul>
<h3>Upgrade to OpenCV 3.2</h3>

<ul>
<li>Updated core framework and components from OpenCV 3.1 to OpenCV 3.2.</li>
</ul>
<h3>Support for Animated gifs</h3>

<ul>
<li>All gifs are now treated as videos. Each gif will be handled as an MPFVideoJob.</li>
<li>Unanimated gifs are treated as 1-frame videos.</li>
<li>The WFM Media Inspector now populates the <code>media_properties</code> map with a <code>FRAME_COUNT</code> entry (in addition to
  the <code>DURATION</code> and <code>FPS</code> entries).</li>
</ul>
<h3>Caffe Component</h3>

<ul>
<li>Added support for the Yahoo Not Suitable for Work (NSFW) Caffe model for explicit material detection.</li>
<li>Updated the Caffe component to support the OpenCV 3.2 Deep Neural Network (DNN) module.</li>
</ul>
<h3>Future Support for Streaming Video</h3>

<blockquote>
<p><strong>NOTE:</strong> At this time, OpenMPF does not support streaming video. This section details what's being / has been done so far to prepare for that feature.</p>
</blockquote>
<ul>
<li>The codebase is being updated / refactored to support both the current "batch" job functionality and new "streaming"
  job functionality.<ul>
<li>batch job: complete video files are written to disk before they are processed</li>
<li>streaming job: video frames are read from a streaming endpoint (such as RTSP) and processed in near real time</li>
</ul>
</li>
<li>The REST API is being updated with endpoints for streaming jobs:<ul>
<li><code>[POST] /rest/streaming/jobs</code>: Creates and submits a streaming job</li>
<li><code>[POST] /rest/streaming/jobs/{id}/cancel</code>: Cancels a streaming job</li>
<li><code>[GET] /rest/streaming/jobs/{id}</code>: Gets information about a streaming job</li>
</ul>
</li>
<li>The Redis and mySQL databases are being updated to support streaming video jobs.<ul>
<li>A batch job will never have the same id as a streaming job. The integer ids will always be unique.</li>
</ul>
</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>The MOG and SuBSENSE component services could segfault and terminate if the <code>USE_MOTION_TRACKING</code> property was set to
  1 and a detection was found close to the edge of the frame. Specifically, this would only happen if the video had a
  width and/or height dimension that was not an exact power of two.<ul>
<li>The reason was because the code downsamples each frame by a power of two and rounds the value of the width and
  height up to the nearest integer. Later on when upscaling detection rectangles back to a size thats relative to
  the original image, the resized rectangle sometimes extended beyond the bounds of the original frame.</li>
</ul>
</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li>If a job is submitted through the REST API, and a user to logged into the web UI and looking at the job status page,
  the WFM may generate "Error retrieving the SingleJobInfo model for the job with id" messages.<ul>
<li>This is because the job status is only added to the HTTP session object if the job is submitted through the web
  UI. When the UI queries the job status it inspects this object.</li>
<li>This message does not appear if job status is obtained using the <code>[GET] /rest/jobs/{id}</code> endpoint.</li>
</ul>
</li>
<li>The <code>[GET] /rest/jobs/stats</code> endpoint aggregates information about all of the jobs ever run on the system. If
  thousands of jobs have been run, this call could take minutes to complete. The code should be improved to execute a
  direct mySQL query.</li>
</ul>
<h2>0.9.0: April 2017</h2>

<blockquote>
<p><strong>WARNING:</strong> MPFImageReader has been disabled in this version of OpenMPF. Component developers should use MPFVideoCapture instead. This affects components developed against previous versions of OpenMPF and components developed against this version of OpenMPF. Please refer to the Known Issues section for more information.</p>
<p><strong>WARNING:</strong> The OALPR Text Detection Component has been renamed to OALPR <strong>License Plate</strong> Text Detection Component. This affects the name of the component package and the name of the actions, tasks, and pipelines. When upgrading from R0.8 to R0.9, if the old OALPR Text Detection Component is installed in R0.8 then you will be prompted to install it again at the end of the upgrade path script. We recommend declining this prompt because the old component will conflict with the new component.</p>
<p><strong>WARNING:</strong> Action, task, and pipeline names that started with <code>MOTION DETECTION PREPROCESSOR</code> have been renamed <code>MOG MOTION DETECTION PREPROCESSOR</code>. Similarly, <code>WITH MOTION PREPROCESSOR</code> has changed to <code>WITH MOG MOTION PREPROCESSOR</code>.</p>
</blockquote>
<h3>Documentation</h3>

<ul>
<li>Updated the <a href="../REST-API/index.html">REST API</a> to reflect job properties, algorithm-specific properties, and
  media-specific properties.</li>
<li>Streamlined the <a href="../CPP-Batch-Component-API/index.html">C++ Batch Component API</a> document for clarity and simplicity.</li>
<li>Completed the <a href="../Java-Batch-Component-API/index.html">Java Batch Component API</a> document.</li>
<li>Updated the <a href="../Admin-Guide/index.html">Admin Guide</a> and <a href="../User-Guide/index.html">User Guide</a> to reflect web UI changes.</li>
<li>Updated the <a href="../Build-Environment-Setup-Guide/index.html">Build Guide</a> with instructions for GitHub repositories.</li>
</ul>
<h3>Workflow Manager</h3>

<ul>
<li>Added support for job properties, which will override pre-defined pipeline properties.</li>
<li>Added support for algorithm-specific properties, which will apply to a single stage of the pipeline and will override
  job properties and pre-defined pipeline properties.</li>
<li>Added support for media-specific properties, which will apply to a single piece and media and will override job
  properties, algorithm-specific properties, and pre-defined pipeline properties.</li>
<li>Components can now be automatically registered and installed when the web application starts in Tomcat.</li>
</ul>
<h3>Web User Interface</h3>

<ul>
<li>The "Close All" button on pop-up notifications now dismisses all notifications from the queue, not just the visible
  ones.</li>
<li>Job completion notifications now only appear for jobs created during the current login session instead of all jobs.</li>
<li>The <code>ROTATION</code>, <code>HORIZONTAL_FLIP</code>, and <code>SEARCH_REGION_*</code> properties can be set using the web interface when creating a
  job. Once files are selected for a job, these properties can be set individually or by groups of files.</li>
<li>The Node and Process Status page has been merged into the Node Configuration page for simplicity and ease of use.</li>
<li>The Media Markup results page has been merged into the Job Status page for simplicity and ease of use.</li>
<li>The File Manager UI has been improved to handle large numbers of files and symbolic links.</li>
<li>The side navigation menu is now replaced by a top navigation bar.</li>
</ul>
<h3>REST API</h3>

<ul>
<li>Added an optional jobProperties object to the <code>/rest/jobs/</code> request which contains String key-value pairs which
  override the pipeline's pre-configured job properties.</li>
<li>Added an optional algorithmProperties object to the <code>/rest/jobs/</code> request which can be used to configure properties
  for specific algorithms in the pipeline. These properties override the pipeline's pre-configured job properties. They
  also override the values in the jobProperties object.</li>
<li>Updated the <code>/rest/jobs/</code> request to add more detail to media, replacing a list of mediaUri Strings with a list of
  media objects, each of which contains a mediaUri and an optional mediaProperties map. The mediaProperties map can be
  used to configure properties for the specific piece of media. These properties override the pipeline's pre-configured
  job properties, values in the jobProperties object, and values in the algorithmProperties object.</li>
<li>Streamlined the actions, tasks, and pipelines endpoints that are used by the web UI.</li>
</ul>
<h3>Flipping, Rotation, and Region of Interest</h3>

<ul>
<li>The <code>ROTATION</code>, <code>HORIZONTAL_FLIP</code>, and <code>SEARCH_REGION_*</code> properties will no longer appear in the detectionProperties
  map in the JSON detection output object. When applied to an algorithm these properties now appear in the
  pipeline.stages.actions.properties element. When applied to a piece of media these properties will now appear in the
  the media.mediaProperties element.</li>
<li>The OpenMPF now supports multiple regions of interest in a single media file. Each region will produce tracks
  separately, and the tracks for each region will be listed in the JSON output as if from a separate media file.</li>
</ul>
<h3>Component API</h3>

<ul>
<li>Java Batch Component API is functionally complete for third-party development, with the exception of Component Adapter
  and frame transformation utilities classes.</li>
<li>Re-architected the Java Batch Component API to use a more traditional Java method structure of returning track lists
  and throwing exceptions (rather than modifying input track lists and returning statuses), and encapsulating job
  properties into MPFJob objects:<ul>
<li><code>List&lt;MPFVideoTrack&gt; getDetections(MPFVideoJob job) throws MPFComponentDetectionError</code></li>
<li><code>List&lt;MPFAudioTrack&gt; getDetections(MPFAudioJob job) throws MPFComponentDetectionError</code></li>
<li><code>List&lt;MPFImageLocation&gt; getDetections(MPFImageJob job) throws MPFComponentDetectionError</code></li>
</ul>
</li>
<li>Created examples for the Java Batch Component API.</li>
<li>Reorganized the Java and C++ component source code to enable component development without the OpenMPF core, which
  will simplify component development and streamline the code base.</li>
</ul>
<h3>JSON Output Objects</h3>

<ul>
<li>The JSON output object for the job now contains a jobProperties map which contains all properties defined for the job
  in the job request. For example, if the job request specifies a <code>CONFIDENCE_THRESHOLD</code> of then the jobProperties map
  in the output will also list a <code>CONFIDENCE_THRESHOLD</code> of 5.</li>
<li>The JSON output object for the job now contains a algorithmProperties element which contains all algorithm-specific
  properties defined for the job in the job request. For example, if the job request specifies a <code>FRAME_INTERVAL</code> of 2
  for FACECV then the algorithmProperties element in the output will contain an entry for "FACECV" and that entry will
  list a <code>FRAME_INTERVAL</code> of 2.</li>
<li>Each JSON media output object now contains a mediaProperties map which contains all media-specific properties defined
  by the job request. For example, if the job request specifies a <code>ROTATION</code> of 90 degrees for a single piece of media
  then the mediaProperties map for that piece of piece will list a <code>ROTATION</code> of 90.</li>
<li>The content of JSON output objects are now organized by detection type (e.g. MOTION, FACE, PERSON, TEXT, etc.) rather
  than action type.</li>
</ul>
<h3>Caffe Component</h3>

<ul>
<li>Added support for flip, rotation, and cropping to regions of interest.</li>
<li>Added support for returning multiple classifications per detection based on user-defined settings. The classification
  list is in order of decreasing confidence value.</li>
</ul>
<h3>New Pipelines</h3>

<ul>
<li>New SuBSENSE motion preprocessor pipelines have been added to components that perform detection on video.</li>
</ul>
<h3>Packaging and Deployment</h3>

<ul>
<li><code>Actions.xml</code>, <code>Algorithms.xml</code>, <code>nodeManagerConfig.xml</code>, <code>nodeServicesPalette.json</code>, <code>Pipelines.xml</code>, and <code>Tasks.xml</code>
  are no longer stored within the Workflow Manager WAR file. They are now stored under <code>$MPF_HOME/data</code>. This makes it
  easier to upgrade the Workflow Manager and makes it easier for users to access these files.</li>
<li>Each component can now be optionally installed and registered during deployment. Components not registered are set to
  the <code>UPLOADED</code> state. They can then be removed or registered through the Component Registration page.</li>
<li>Java components are now packaged as tar.gz files instead of RPMs, bringing them into alignment with C++ components.</li>
<li>OpenMPF R0.9 can be installed over OpenMPF R0.8. The deployment scripts will determine that an upgrade should take
  place.<ul>
<li>After the upgrade, user-defined actions, tasks, and pipelines will have "CUSTOM" prepended to their name.</li>
<li>The job_request table in the mySQL database will have a new "output_object_version" column. This column will
  have "1.0" for jobs created using OpenMPF R0.8 and "2.0" for jobs created using OpenMPF R0.9. The JSON output
  object schema has changed between these versions.</li>
</ul>
</li>
<li>Reorganized source code repositories so that component SDKs can be downloaded separately from the OpenMPF core and so
  that components are grouped by license and maturity. Build scripts have been created to streamline and simplify the
  build process across the various repositories.</li>
</ul>
<h3>Upgrade to OpenCV 3.1</h3>

<ul>
<li>The OpenMPF software has been ported to use OpenCV 3.1, including all of the C++ detection components and the markup
  component. For the OpenALPR license plate detection component, the versions of the openalpr, tesseract, and leptonica
  libraries were also upgraded to openalpr-2.3.0, tesseract-3.0.4, and leptonica-1.7.2. For the SuBSENSE motion
  component, the version of the SuBSENSE library was upgraded to use the code found at this
  location: <a href="https://bitbucket.org/pierre_luc_st_charles/subsense/src">https://bitbucket.org/pierre_luc_st_charles/subsense/src</a>.</li>
</ul>
<h3>Bug Fixes</h3>

<ul>
<li>MOG motion detection always detected motion in frame 0 of a video. Because motion can only be detected between two
  adjacent frames, frame 1 is now the first frame in which motion can be detected.</li>
<li>MOG motion detection never detected motion in the first frame of a video segment (other than the first video segment
  because of the frame 0 bug described above). Now, motion is detected using the first frame before the start of a
  segment, rather than the first frame of the segment.</li>
<li>The above bugs were also present in SuBSENSE motion detection and have been fixed.</li>
<li>SuBSENSE motion detection generated tracks where the frame numbers were off by one. Corrected the frame index logic.</li>
<li>Very large video files caused an out of memory error in the system during Workflow Manager media inspection.</li>
<li>A job would fail when processing images with an invalid metadata tag for the camera flash setting.</li>
<li>Users were permitted to select invalid file types using the File Manager UI.</li>
</ul>
<h3>Known Issues</h3>

<ul>
<li><strong>MPFImageReader does not work reliably with the current release version of OpenCV 3.1</strong>: In OpenCV 3.1, new
  functionality was introduced to interpret EXIF information when reading jpeg files.</li>
<li>There are two issues with this new functionality that impact our ability to use the OpenCV <code>imread()</code> function with
  MPFImageReader:<ul>
<li>First, because of a bug in the OpenCV code, reading a jpeg file that contains exif information could cause it to
  hang. (See <a href="https://github.com/opencv/opencv/issues/6665">https://github.com/opencv/opencv/issues/6665</a>.)</li>
<li>Second, it is not possible to tell the <code>imread()</code>function to ignore the EXIF data, so the image it returns is
  automatically rotated. (See <a href="https://github.com/opencv/opencv/issues/6348">https://github.com/opencv/opencv/issues/6348</a>.) This results in the MPFImageReader
  applying a second rotation to the image due to the EXIF information.</li>
</ul>
</li>
<li>To address these issues, we developed the following workarounds:<ul>
<li>Created a version of the MPFVideoCapture that works with an MPFImageJob. The new MPFVideoCapture can pull frames
  from both video files and images. MPFVideoCapture leverages cv::VideoCapture, which does not have the two issues
  described above.</li>
<li>Disabled the use of MPFImageReader to prevent new users from trying to develop code leveraging this previous
  functionality.</li>
</ul>
</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../License-And-Distribution/index.html" class="btn btn-neutral float-right" title="License and Distribution">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../index.html" class="btn btn-neutral" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>OpenMPF</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../index.html" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../License-And-Distribution/index.html" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js"></script>
      <script src="../js/mustache.js"></script>
      <script src="../search/require.js"></script>
      <script src="../search/search.js"></script>

</body>
</html>
